[
  {
    "objectID": "tils/fastai_ch2/ch2.html",
    "href": "tils/fastai_ch2/ch2.html",
    "title": "Fastai Chapter 2",
    "section": "",
    "text": "Watch the lecture once.\nRun the Bear Classifier notebook & figure out how all the pieces fit together.\nReproduce the Bear classifier model + clean the data + re-train the model\nDeploy the Bear model on HF spaces & try the model\nRead the chapter 2 fastbook\n\nReview the questionare.\nRead the chapter 2 + take book notes using cornell method.\nTry to answer the questionare.\n\nCreate another notebook using a different dataset.\nWatch the lecture again + take lecture notes.\nFinish the “Questionare + Further research”\nFinish AIquizzes - Lesson 2\nLearn any concepts/lib used in the lecture\n\na\nb\nFinish Lesson 1 TODOs also\n\nRead the Fastai Forum discussion on lesson 2\n\nTake imp/helpful notes,ideas from discussions.\nDo the homeworks of Radek, Jeremy, Sanyam, Tanishq etc…\n\nWalk with Fastai Lesson 1 - all 3 parts (optional)\nComputer vision intro – fastai\n\nTry to recreate these models - without looking at it\nCreate notebooks but use a different dataset\n\nankify notes\n\n\n\n\n\n\nCode\nimport fastbook\nfastbook.setup_book()\n\nfrom fastbook import *\n\n\n\n\nCode\nfrom fastai.vision.all import *\n\n\n\n\nCode\nfrom fastcore.all import *\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere do text models currently have a major deficiency?\n\na\nb\nc\nd\n\n\n\n\nWhat are possible negative societal implications of text generation models?\n\n\n\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n\n\n\nWhat kind of tabular data is deep learning particularly good at?\n\n\n\nWhat’s a key downside of directly using a deep learning model for recommendation systems?\n\n\n\nWhat are the steps of the Drivetrain Approach?\n\n\n\nHow do the steps of the Drivetrain Approach map to a recommendation system?\n\n\n\nCreate an image recognition model using data you curate, and deploy it on the web.\n\n\n\nWhat is DataLoaders?\n\n\n\nWhat four things do we need to tell fastai to create DataLoaders?\n\n\n\nWhat does the splitter parameter to DataBlock do?\n\n\n\nHow do we ensure a random split always gives the same validation set?\n\n\n\nWhat letters are often used to signify the independent and dependent variables?\n\n\n\nWhat’s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?\n\n\n\nWhat is data augmentation? Why is it needed?\n\n\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\n\n\n\nWhat is the difference between item_tfms and batch_tfms?\n\n\n\nWhat is a confusion matrix?\n\n\n\nWhat does export save?\n\n\n\nWhat is it called when we use a model for making predictions, instead of training?\n\n\n\nWhat are IPython widgets? 22. When would you use a CPU for deployment? When might a GPU be better?\n\n\n\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n\n\n\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice? 25. What is out-of-domain data?\n\n\n\nWhat is domain shift?\n\n\n\nWhat are the three steps in the deployment process?\n\na\nb\nc\n\n\n\n\n\n\nConsider how the Drivetrain Approach maps to a project or problem you’re interested in.\n\n\n\nWhen might it be best to avoid certain types of data augmentation?\n\n\n\nFor a project you’re interested in applying deep learning to, consider the thought experiment, “What would happen if it went really, really well?”\n\n\n\nStart a blog and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you’re interested in.\n\n\n\n\n\n\nNotes, Experiments, Q&As important / related to this lesson.\n\n\n\n\n\nExploring Fastai Docs Tutorials.\nExperiments I did.\nLearnings/Quirks I had.\n\n\n\n\n\nFinish a mini-project based on this lesson.\n\nKaggle dataset/problem\nImplement Lesson Notebook with different dataset."
  },
  {
    "objectID": "tils/fastai_ch2/ch2.html#todo",
    "href": "tils/fastai_ch2/ch2.html#todo",
    "title": "Fastai Chapter 2",
    "section": "",
    "text": "Watch the lecture once.\nRun the Bear Classifier notebook & figure out how all the pieces fit together.\nReproduce the Bear classifier model + clean the data + re-train the model\nDeploy the Bear model on HF spaces & try the model\nRead the chapter 2 fastbook\n\nReview the questionare.\nRead the chapter 2 + take book notes using cornell method.\nTry to answer the questionare.\n\nCreate another notebook using a different dataset.\nWatch the lecture again + take lecture notes.\nFinish the “Questionare + Further research”\nFinish AIquizzes - Lesson 2\nLearn any concepts/lib used in the lecture\n\na\nb\nFinish Lesson 1 TODOs also\n\nRead the Fastai Forum discussion on lesson 2\n\nTake imp/helpful notes,ideas from discussions.\nDo the homeworks of Radek, Jeremy, Sanyam, Tanishq etc…\n\nWalk with Fastai Lesson 1 - all 3 parts (optional)\nComputer vision intro – fastai\n\nTry to recreate these models - without looking at it\nCreate notebooks but use a different dataset\n\nankify notes"
  },
  {
    "objectID": "tils/fastai_ch2/ch2.html#notebook-excercise",
    "href": "tils/fastai_ch2/ch2.html#notebook-excercise",
    "title": "Fastai Chapter 2",
    "section": "",
    "text": "Code\nimport fastbook\nfastbook.setup_book()\n\nfrom fastbook import *\n\n\n\n\nCode\nfrom fastai.vision.all import *\n\n\n\n\nCode\nfrom fastcore.all import *"
  },
  {
    "objectID": "tils/fastai_ch2/ch2.html#book-notes",
    "href": "tils/fastai_ch2/ch2.html#book-notes",
    "title": "Fastai Chapter 2",
    "section": "",
    "text": "Where do text models currently have a major deficiency?\n\na\nb\nc\nd\n\n\n\n\nWhat are possible negative societal implications of text generation models?\n\n\n\nIn situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?\n\n\n\nWhat kind of tabular data is deep learning particularly good at?\n\n\n\nWhat’s a key downside of directly using a deep learning model for recommendation systems?\n\n\n\nWhat are the steps of the Drivetrain Approach?\n\n\n\nHow do the steps of the Drivetrain Approach map to a recommendation system?\n\n\n\nCreate an image recognition model using data you curate, and deploy it on the web.\n\n\n\nWhat is DataLoaders?\n\n\n\nWhat four things do we need to tell fastai to create DataLoaders?\n\n\n\nWhat does the splitter parameter to DataBlock do?\n\n\n\nHow do we ensure a random split always gives the same validation set?\n\n\n\nWhat letters are often used to signify the independent and dependent variables?\n\n\n\nWhat’s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?\n\n\n\nWhat is data augmentation? Why is it needed?\n\n\n\nProvide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.\n\n\n\nWhat is the difference between item_tfms and batch_tfms?\n\n\n\nWhat is a confusion matrix?\n\n\n\nWhat does export save?\n\n\n\nWhat is it called when we use a model for making predictions, instead of training?\n\n\n\nWhat are IPython widgets? 22. When would you use a CPU for deployment? When might a GPU be better?\n\n\n\nWhat are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?\n\n\n\nWhat are three examples of problems that could occur when rolling out a bear warning system in practice? 25. What is out-of-domain data?\n\n\n\nWhat is domain shift?\n\n\n\nWhat are the three steps in the deployment process?\n\na\nb\nc\n\n\n\n\n\n\nConsider how the Drivetrain Approach maps to a project or problem you’re interested in.\n\n\n\nWhen might it be best to avoid certain types of data augmentation?\n\n\n\nFor a project you’re interested in applying deep learning to, consider the thought experiment, “What would happen if it went really, really well?”\n\n\n\nStart a blog and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you’re interested in."
  },
  {
    "objectID": "tils/fastai_ch2/ch2.html#fastai-forumdiscord",
    "href": "tils/fastai_ch2/ch2.html#fastai-forumdiscord",
    "title": "Fastai Chapter 2",
    "section": "",
    "text": "Notes, Experiments, Q&As important / related to this lesson."
  },
  {
    "objectID": "tils/fastai_ch2/ch2.html#tils",
    "href": "tils/fastai_ch2/ch2.html#tils",
    "title": "Fastai Chapter 2",
    "section": "",
    "text": "Exploring Fastai Docs Tutorials.\nExperiments I did.\nLearnings/Quirks I had."
  },
  {
    "objectID": "tils/fastai_ch2/ch2.html#mini-project",
    "href": "tils/fastai_ch2/ch2.html#mini-project",
    "title": "Fastai Chapter 2",
    "section": "",
    "text": "Finish a mini-project based on this lesson.\n\nKaggle dataset/problem\nImplement Lesson Notebook with different dataset."
  },
  {
    "objectID": "tils/new_til/index.html",
    "href": "tils/new_til/index.html",
    "title": "How to Create a Custom Section in Quarto",
    "section": "",
    "text": "Today I learned how to create custom blog sections in quarto blogsite (eg: TILs )\n\nCreate tils.qmd in root directory of quarto project\n\n\n\nCreate a tils folder in root directory\n\nNow you can add new TIL posts as sub-folders & .qmd files within them.\n\n\nUpdate quarto yaml file"
  },
  {
    "objectID": "tils.html",
    "href": "tils.html",
    "title": "TILs",
    "section": "",
    "text": "Favorite Quotes\n\n\n\n\n\n\nquotes\n\n\n\nFavourite Quotes\n\n\n\n\n\nJun 22, 2025\n\n\nMy Idols\n\n\n\n\n\n\n\n\n\n\n\n\nFastai Chapter 2\n\n\n\n\n\n\nFastai\n\n\nnotes\n\n\n\nA summary of Chapter 2 from the Fastai book, covering the basics of deep learning and neural networks.\n\n\n\n\n\nJun 17, 2024\n\n\nShraey\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Custom Section in Quarto\n\n\n\n\n\n\nquarto\n\n\nblog\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blogs",
    "section": "",
    "text": "How to study fastai\n\n\n\n\n\n\nguide\n\n\nfastai\n\n\n\n\n\n\n\n\n\nJun 17, 2025\n\n\nFastai Community, Shraey\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fastai-guide/index.html",
    "href": "posts/fastai-guide/index.html",
    "title": "How to study fastai",
    "section": "",
    "text": "You are not expected to remember everything. Yet.\nYou are not expected to Understand everything. Yet.\nYou are not expected to know Why everything works. Yet.\nJust be in situation where you can enter the code, run it, and something happening and you can start to experiment and get a feel for what’s going on & then push on.\n\nLook at the inputs , Look at the outputs. Experiment with Code.\n\nMost of the successful Fastai students have watched the videos 3 times."
  },
  {
    "objectID": "posts/fastai-guide/index.html#things-jeremy-says-to-do-in-fastai",
    "href": "posts/fastai-guide/index.html#things-jeremy-says-to-do-in-fastai",
    "title": "How to study fastai",
    "section": "Things Jeremy Says to do (in fastai) :",
    "text": "Things Jeremy Says to do (in fastai) :\n\nListen to Jeremy — (said radek)\nI mean like really listen, to everything  and spend your time how he suggests”. If he says, go code — do it. If he says “try blah”… go try blah.\nMost consistent regret: “I should have listened when Jeremy said don’t spend hours lost in theory trying to understand everything right away”.\n\n\nLesson 1\n\nDon’t try to stop and understand everything.\nDon’t waste your time, learn Jupyter keyboard shortcuts. Learn 4 to 5 each day.\nPlease run the code, really run the code. Don’t go deep on theory. Play with the code, see what goes in and what comes out.\nPick one project. Do it really well. Make it fantastic.\nRun this notebook (lesson1-pets.ipynb), but then get your own dataset and run it! (extra emphasis: do this!)\nIf you have a lot of categories, don’t run confusion matrix, run… interp.most_confused(min_val=n)\n\n\n\nLesson 2\n\nIf forum posts are overwhelming, click “summarize this topic” at the bottom of the first post.\nIt’s okay to feel intimidated, there’s a lot, but just pick one piece and dig into it. Try to push a piece of code, or learn a concept like regular expressions, or create a classifier, or whatever. \nIf you’re struck, keep going.\nIf you’re not sure which learning rate is best from plot, try both and see.\nWhen you put a model into production, you probably want to use CPU for inference, except at massive scale. Context: Lesson 2: Putting Model into Production 50\nMost organizations spend too much time gathering data. Get a small amount first, see how it goes.\n\n\n\nLesson 3\n\nIf you use a dataset, it would be very nice of you to cite the creator and thank them for their dataset.\nThis week, see if you can come up with a problem that you would like to solve that is either multi-label classification or image regression or image segmentation or something like that and see if you can solve that problem. Context: Fast.ai Lesson 3 Homework 115\nAlways use the same stats that the model was trained with. Context: Lesson 3: Normalized data and ImageNet 22\nIn response to “Is there a reason you shouldn’t deliberately make lots of smaller datasets to step up from in tuning, let’s say 64x64 to 128x128 to 256x256?”: Yes you should totally do that, it works great, try it! Context: Lesson 3: 64x64 vs 128x128 vs 256x256 50\n\n\n\nLesson 4\n\nIf you’re doing NLP stuff, make sure you use all of the text you have (including unlabeled validation set) to train your model, because there’s no reason not to. Lesson 4: A little NLP trick 60\nIn response to “What are the 10% of cases where you would not use neural nets”. You may as well try both. Try a random forest and try a neural net. Lesson 4: How to know when to use neural nets 48\nUse these terms (parameters, layers, activations…etc) and use them accurately. Lesson 4: Important vocabulary for talking about ML 44\n\n\n\nLesson 5\n\nThe answer to the question “Should I try blah?” is to try blah and see, that’s how you become a good practitioner. Lesson 5: Should I try blah? 27\nIf you want to play around, try to create your own nn.linear class. You could create something called My_Linear and it will take you, depending on your PyTorch experience, an hour or two. We don’t want any of this to be magic and you know everything necessary to create this now. These are the things you should be doing for assignments this week, not so much new applications but trying to write more of these things from scratch and get them to work. Learn how to debug them and check them to see what’s going in and coming out. Lesson 5 Assignment: Create your own version of nn.linear 18\nA great assignment would be to take Lesson 2 SGD and try to add momentum to it. Or even the new notebook we have for MNIST, get rid of the Optim.SGD and write your own update function with momentum Lesson 5: Another suggested assignment 14\n\n\n\nLesson 6\n\nNot an explicit “do this” but it feels like it fits here. “One of the big opportunities for research is to figure out how to do data augmentation for different domains. Almost nobody is looking at that and to me it is one of the biggest opportunities that could let you decrease data requirements by 5-10x.” Lesson 6: Data augmentation on inputs that aren’t images 37\nIf you take your time going through the convolution kernel section and the heatmap section of this notebook, running those lines of code and changing them around a bit. The most important thing to remember is shape (rank and dimensions of tensor). Try to think “why?”. Try going back to the printout of the summary, the list of the actual layers, the picture we drew and think about what’s going on. Lesson 6: Go through the convolution kernel and heatmap notebook 17\n\n\n\nLesson 7\n\nNot an explicit “do this” but it feels like it fits here. “One of the big opportunities for research is to figure out how to do data augmentation for different domains. Almost nobody is looking at that and to me it is one of the biggest opportunities that could let you decrease data requirements by 5-10x.” Lesson 6: Data augmentation on inputs that aren’t images 37\nIf you take your time going through the convolution kernel section and the heatmap section of this notebook, running those lines of code and changing them around a bit. The most important thing to remember is shape (rank and dimensions of tensor). Try to think “why?”. Try going back to the printout of the summary, the list of the actual layers, the picture we drew and think about what’s going on. Lesson 6: Go through the convolution kernel and heatmap notebook 17\n\n\n\nLesson 8\n\nDon’t let this lesson intimidate you. It’s meant to be intense in order to give you ideas to keep you busy before part two comes out.\n\nParts 2-5 come from a great speech towards the end of the lesson. I’d highly recommend revisiting here: Lesson 7: What to do once you’ve completed Part 1 34\n\nGo back and watch the videos again. There will be bits where you now understand stuff you didn’t before.\nWrite code and put it on GitHub. It doesn’t matter if it’s great code or not, writing it and sharing it is enough. You’ll get feedback from your peers that will help you improve.\nIt’s a good time to start reading some of the papers introduced in the course. All the parts that say derivations/theorems/lemmas, feel free to skip, they will add nothing to your understanding of practical deep learning. Read the parts where they talk about why they are solving this problem and the results. Write summaries that will explain this to you of 6 months ago.\nPerhaps the most important is to get together with others. Learning works a lot better if you have that social experience. Start a book club, a study group, get involved in meetups, and build things. It doesn’t have to be amazing. Build something that will make the world slightly better, or will be slightly delightful to your two year old to see it. Just finish something, and then try to make it a bit better. Or get involved with fast.ai and helping develop the code and documentation. Check Dev Projects Index on forums.\nIn response to “What would you recommend doing/learning/practicing until the part 2 course starts?” “Just code. Just code all the time. Look at the shape of your inputs and outputs and make sure you know how to grab a mini-batch. There’s so much material that we’ve covered, if you can get to a pointwhere you can rebuild those notebooks from scratch without cheating too much, you’ll be in the top echelon of practitioners and you’ll be able to do all\nof these things yourself and that’s really really rare. Lesson 7: What to do/learn/practice between now and Part 2 31\n\n\n\nLesson 8 (part 2 , 2019)\n\nThe cutting edge of deep learning is really about engineering, not about papers. The difference between really effective people in deep learning and the rest is really about who can make things in code that work properly and there’s very few of those people. The Cutting Edge 24\nThere are many opportunities for you in this class. Experiment lots, particularly in your domain area. Write stuff down for the you of six months ago, that’s your audience. It doesn’t have to be perfect. The Opportunities are Much Greater Now 4\nIf you don’t understand something from Part 1, go back and watch the video about that thing. Don’t just keep blasting forwards. Especially the later lessons of Part 1 where we went into more detail. It’s Assumed You Understand Part 1 1\nOverfit -&gt; Reduce Overfitting -&gt; There is no step 3. Remember overfitting doesn’t mean having a lower training loss than validation loss, that is normal. It means you have seen your validation error getting worse, until you see that happening you’re not overfitting. 3 Steps to Training a Really Good Model 5\nLearn to pronounce Greek letters. A big part of reading papers is getting past the fear of Greek letters. It’s very hard to read something you can’t pronounce. You’re saying to yourself “squiggle bracket 1+squiggle G squiggle 1-squiggle. Time to Start Reading Papers 5\nGet very used to…\n\nPyTorch tensors\n‘.view()’, this is how we reshape vectors (e.g from 784 -&gt; 28*28)\nMatrix multiplications - Resource for Visualizing Matrix Multiplication 2\nThis equation c[i,j] += a[i,k] * b[k,j]  Equation Context 3\n\nHomework: Take our most mind-bending broadcast and convince yourself of why it works. Put it in Excel, or do it on paper if it’s not already clear to you why this works. Mind Bending Broadcast 9\nApply the simple broadcasting rules when working with higher ranked tensors. Don’t try to keep it all in your head. (rules are listed in notebook) Apply the Broadcasting Rules 1\nAlways make sure your validation and training set were normalized in the same way.\nReading papers from competition winners is a very very good idea. Normal Papers vs Competition Papers 20\nMore Homework: Go read section 2.2 of the Resnet Paper 3\nWhen you’re writing your own code, put some comments in your code to let the next person know what the hell you’re doing. Jeremy Finds Undocumented Suboptimal Constant in PyTorch\nMore Homework: If you don’t remember the chain rule, go to Khan Academy, they have a great tutorial on the chain rule 3\n\n\n\nLesson 9\n\nWhen you see something in a library, don’t assume that it’s right or that it makes sense. When it comes to deep learning, none of us know what we’re doing. It doesn’t take too much to dig into it yourself. Lesson 9: Don’t assume libraries are correct 4\nI dont set a random seed, this is very intentional because I want to see variation in my model, I don’t want it to be hidden away behind a fixed seed. Lesson 9: When not to use a seed 3\nCoroutines in python are worth looking up and learning about, we’ll be using them lots.\nYou can and should schedule everything, your dropout amount, what kind of data augmentation you do, weight decay, learning rate, momentum, everything. It’s very unlikely you would want the same hyperparameters throughout. Lesson 9: Hyperparameter Scheduling 3\n\n\n\nLesson 10\n\nI want to remind you that it’s totally okay if you’re not keeping up with everything, I’m trying to give you enough to keep you busy until Part 2 next year, but don’t feel like you need to understand everything within a week of first hearing it. Lesson 10: Don’t worry 1\nReminder: This was also advice from Part 1 Lesson 2 1. If you’re here you got through that and you’ll get through this too!\nWhat will tend to happen with stuff in fast.ai is that we’ll start with something trivially easy and at some point in the next hour or two you might reach a point where you’re feeling totally lost, the trick is to go back to the point where it was trivially easy and figure out the bit where you suddenly noticed you were totally lost and find the bit in the middle where you kind of missed a bit because we are going to keep building up from trivially easy stuff. Lesson 10: Where’d you get lost?\nDunder methods, there’s a particular list I suggest you know, and this is the list: Lesson 10: Dunder Methods 5\n\n\n__getitem__\n__getattr__\n__setattr__\n__del__\n__init__\n__new__\n__enter__\n__exit__\n__len__\n__repr__\n__str__\n\n\nYou need to be really good at browsing source code. This is a list of things you should know how to do in your editor of choice: Lesson 10: What an editor needs 4\n\n\nJump to tag/symbol by with(with completions)\nJump to current tag\nJump to library tags\nGo back\nSearch\nOutlining/folding\n\n\nStandard deviation is more sensitive to outliers than mean absolute deviation. For that reason the mean absolute deviation is very often the thing you want to be using because, in ML, outliers are often more of a problem than a help. Mean absolute deviation is really underused, you should get used to it. Lesson 10: Mean absolute deviation 1\nReplacing things involving squares with things that use absolute values often works better. It’s a good tip to remember. Lesson 10: Squares are for squares\nFrom now on, you’re not allowed to look at an equation, or type it in LaTeX without also typing it in python, actually calculating some values, and plotting it, because this is the only way we get a sense of what these variables and equations actually mean. Lesson 10: Always play with equations\nYour problem to play with during the week is “how accurate can you make a model just using the layers we’ve created so far, and, for the ones that are great accuracy, what does the telemetry look like? How can you tell it’s going to be good? And then what insights can\nyou gain from that to make it even better? Try to beat me (0.9898 at best point, 0.9822 at end) You can beat it pretty easily with some playing around, but do some experiments. Lesson 10: Toy Problem #1 (Homework) 2\nEpsilon appears in lots of places in deep learning and is a fantastic hyperparameter that you should be using to train things better Lesson 10: More about epsilon 5\nIt’s really good to create interesting little games to play, in research we call them toy problems. Almost everything in research is toy problems you come up with and try to find solutions to. Another toy problem to try during the week is, what’s the best accuracy you can get in a single epoch using whatever normalization you like and only architectures we’ve used up to lesson 7?  Lesson 10: Toy Problem #2 (Homework) 3\n\n\n\nLesson 11\n\nA big part of getting good at deep learning in your domain is knowing how to create small workable useful datasets. In your domain area, whether it’s audio or sanskrit texts, try to come up with a toy problem or two which you hope might give insight into your full problem. Lesson 11: Small useful datasets 4\nIf you haven’t seen compose used in programming before, google it. It’s a super useful concept and it comes up all the time. Lesson 11: Compose 10\nA great opportunity to contribute is using telemetry to view activations of different layers and seeing what happens experimentally. The theory people generally don’t know how to train models, and the practitioners forget about actually thinking about the foundations at all, but if you can combine the two and try some experiments to see what happens when you adjust weight decay, you can find some really interesting results. Lesson 11: Another opportunity 2\nThe trick to making Adam and Adam-like things work well is to make epsilon 0.1 (or between 1e-3 and 1e-1). Most people use 1e-7 and that’s never going to be a good idea.\nLesson 11: Another place where epsilon matters 7\nWhen you’re augmenting your data, look at or listen to your augmented data. Don’t just chuck it in a model but look at the augmented output and try to find something to study to see if you’re losing information. Lesson 11: Mindful data augmentation 1\nImage augmentation isn’t just about throwing some transformation functions in there, but think about when you’re going to do it because you have this pipeline where you start with bytes, which become floats. Think about where you’re gonna do the work. Do whatever you can while they’re still bytes, but be careful to not do anything that will cause rounding errors or saturation problems. Lesson 11: Optimizing data augmentation 1\nWhen doing data augmentation on non-images, ask yourself what kind of changes could occur in data that wouldn’t cause the label to change but would still leave that data as a reasonable example that could show up in your dataset. Lesson 11: Data augmentation in other domains\n\n\n\nLesson 12\n\nBe careful about using automated formatting and creating too many rules. Sometimes unconventional formatting can help you to understand your code better. This is the only way to make your code work. Debugging machine learning code is awful so you have to make sure the thing you write makes sense and is simple. Lesson 12: Unconventional Formatting 2\nThe mixup paper is a pretty easy read by paper standards. I suggest you check it out. 3\nMake your equations in code as close as possible to the paper you’re implementing. When you’re comparing something to a paper, you want something that you can look at and straightaway say “oh, that looks very familiar”. As long as it’s not familiar you may want to think about how to make it more familiar. Lesson 12: When to use Greek Letters 2\nDon’t listen to people in your organization saying we can’t start modeling until we do all this cleanup work. Start modeling right now, see if the results are okay, and if they are, maybe you can skip the cleanup or do them both simultaneously.\nRead the Bag of Tricks Paper 8 and think about for each of those resnet tweaks, why did they do that? It wasn’t some brainless random search where they tried everything. They sat back and thought “how do we use all the inputs we have and take advantage of all the computation we are doing?” Spend time thinking about architectures and experimenting with them, it’s really important for effective practitioners to be able to write nice concise architectures so that you can change them and understand them. Lesson 12: Build Architectures Thoughtfully 1\nA super important thing to remember if you’re doing fine-tuning is don’t ever freeze the weights in the batchnorm layers if you’re ever doing partial layer training. Lesson 12: Batchnorm Does Weird Things 2\nThe way to debug in DL is to not make mistakes in the first place. The only way to do that is to make your code so simple that it can’t have a mistake, and to check every single intermediate result along the way. You also need to be a great scientist, which means keeping a journal notebook to keep track of your results. Lesson 12: A War Story About DL Debugging 6 (really long and detailed response that’s worth a second listen)\nWhen preprocessing data for neural nets, leave it as raw as you can is the rule of thumb.\nAnybody who has made it to lesson 12 in this course should be learning Swift for TensorFlow. This one comes with some great advice.\n\n\nPython’s days are numbered.\nDL libraries change all the time so if you’re spending all the time learning one library in one language, you won’t be prepared for that change.\nI’ve spent time using, in real world scenarios, at least a couple dozen languages, and each time I learn a new language, I become a better developer. It’s just a good idea to learn a new language\nIf you’re hacking around over the coming months and you find things aren’t the way you want then you can, and should, change it."
  },
  {
    "objectID": "posts/fastai-guide/index.html#jeremy-howards-advice",
    "href": "posts/fastai-guide/index.html#jeremy-howards-advice",
    "title": "How to study fastai",
    "section": "Jeremy Howard’s advice :",
    "text": "Jeremy Howard’s advice :\n\nTo become a world class DL Practitioner\n\nWilling to learn\nCuriousity\nTenacity\nConsistency (10,000 hrs)\n\n\n\nLearning by Discovery (How to learn DL in Fastai way)\n\n\nIn deep learning, it really helps if you have the motivation to fix your model to get it to do better. That’s when you start learning the relevant theory.\n\nBut you need to have the model in the first place. Learn almost everything through real examples.\nAs we build out those examples, we go deeper and deeper, and we’ll show you how to make your projects better and better.\nThis means that you’ll be gradually learning all the theoretical foundations you need, in context, in such a way that you’ll see why it matters and how it works.\n\n\n\n\nFastai : Lesson 0\n\nSchedule the course - how long will u take to finish each lesson.\n\non which day will u watch the Lesson , what will u do after watching Lesson (next steps) etc. (to avoid procrastination & maintain consistency)\n\nDon’t add too many fillers while doing the course\n\ni need to learn all the numpy and pandas and matplotlib and python before i go to next lesson this will break the flow of your learning fastai\nJust learn enough so that you can search the thing whenever needed.\nif you start learning all the pandas features…it will alone 6 months.\nJust learn fundamentals and carry on training models (dooing DL).\n\nTeaching the whole game\n\nWe’ll start off by showing you how to use a complete, working, usable, state-of-the-art deep learning network to solve real-world problems using simple, expressive tools.\nAnd then we’ll gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on…\n\nThe hardest part of deep learning is artisanal:\n\nHow do you know if you’ve got enough data, whether it is in the right format, if your model is training properly, and, if it’s not, what you should do about it? That is why we believe in learning by doing.\nAs with basic data science skills, with deep learning you get better only through practical experience. Trying to spend too much time on the theory can be counterproductive.\nThe key is to just code and try to solve problems: the theory can come later, when you have context and motivation.\n\nGet started with coding\n\nEven if you are not good at coding -\n\nRather than thinking - it’s a shame “i don’t know coding”\nNow you have a really fun project (fastai) - to learn coding for & from.\n\nLots of people have become great coders while doing the course (Zach Mueller) . You’ll learn lots of CS like -\n\nOOPs, Functional programming\nlist comprehension\ngpu acceleration, software design, etc..\n\n\nDon’t try to go in depth in Fastai library\n\nBecause , If you’ve finshed fastai part 1 , part 2 + live coding + fastbook + walk with fastai  – Then, you’d have implemented all the fastai library (from scratch + all the Deep learning fundamentals.)\n\nUtilize the Clean version of fastbook :\n\nThis notebook contains just code (no context).\nAfter reading the main fastbook , come to this version - see the code and before running the code…\nAsk yourself\n\nwhy is this cell here ?\nwhat’s it for ?\nwhat’s it gonna do ?\nwhat’s the input and output gonna look like ?\n\n\nHow to use the Provided Notebooks\n\nRead through the notebook. If everything makes sense, put it aside and create a new notebook.\nNow try to code the same process as we went through in class.\nIf you get stuck at any point, you can refer to the class notebook. Find the solution to what you are stuck on. Look up the relevant documentation. Put the class notebook aside again, go back to your notebook, and try to code the solution.\nIf you are still stuck, you can refer to the class notebook again. Do not copy and paste the needed code. Instead, type it out yourself. Check that it runs. If so, try changing the inputs, and see if that effects the outputs as you expect.\nAny time that you feel unsure about why a particular step is being done, or how it works, or why the outputs are what you observe (or anything else), please ask on the forums. As I write this (week 3 of the course), there has not been a single question on the forums that has not been resolved!\n\nThere will be times when the journey feels hard.\n\nTimes when you feel stuck. Don’t give up!\nRewind through the book to find the last bit where you definitely weren’t stuck, and then read slowly through from there to find the first thing that isn’t clear.\nThen try some code experiments yourself, and Google around for more tutorials on whatever the issue you’re stuck with is—often you’ll find a different angle on the material that might help it to click.\n\nAlso, it’s expected and normal to not understand everything (especially the code) on first reading.\n\n Trying to understand the material serially before proceeding can sometimes be hard. Sometimes things click into place after you get more context from parts down the road, from having a bigger picture. So if you do get stuck on a section, try moving on anyway and make a note to come back to it later.\n\n\n\n\nYour Projects and Mindset\n\nadd these contents under one “Projects” section!\n\n\n\nTenacity and Deep Learning\n\nThe story of deep learning is one of tenacity and grit by a handful of dedicated researchers. After early hopes (and hype!), neural networks went out of favor in the 1990s and 2000s, and just a handful of researchers kept trying to make them work well. Three of them, Yann Lecun, Yoshua Bengio, and Geoffrey Hinton, were awarded the highest honor in computer science, the Turing Award (generally considered the “Nobel Prize of computer science”), in 2018 after triumphing despite the deep skepticism and disinterest of the wider machine learning and statistics community.\nHinton has told of how academic papers showing dramatically better results than anything previously published would be rejected by top journals and conferences, just because they used a neural network. Lecun’s work on convolutional neural networks, which we will study in the next section, showed that these models could read hand‐ written text—something that had never been achieved before. However, his break‐ through was ignored by most researchers, even as it was used commercially to read 10% of the checks in the US!\nIn addition to these three Turing Award winners, many other researchers have battled to get us to where we are today. For instance, Jurgen Schmidhuber (who many believe should have shared in the Turing Award) pioneered many important ideas, including working with his student Sepp Hochreiter on the long short-term memory (LSTM) architecture (widely used for speech recognition and other text modeling tasks, and used in the IMDb example in Chapter 1). Perhaps most important of all, Paul Werbos in 1974 invented backpropagation for neural networks, the technique shown in this chapter and used universally for training neural networks (Werbos 1994). His devel‐ opment was almost entirely ignored for decades, but today it is considered the most important foundation of modern AI.\nThere is a lesson here for all of us! On your deep learning journey, you will face many obstacles, both technical and (even more difficult) posed by people around you who don’t believe you’ll be successful. There’s one guaranteed way to fail, and that’s to stop trying. We’ve seen that the only consistent trait among every fast.ai student who’s gone on to be a world-class practitioner is that they are all very tenacious.\n\n\n\nBeing tenacious also means …\n\nKeep going (until u finish the course , project , competition)\nKeep coming back & continue the Fastai Lessons (even if u took a break of 2-3 days or 2-3 years)\n\n\n\n\nSelf-Learning Tips\n\nLearn on demand\n\nIf you find you need to know some foundational skill, learn it at that time, and just what you need. A lot of students get lost in rabbit holes of foundational math study and never develop expertise in AI/ML as a result!\n\nDoing an Actual Project by using something new.\n\nI have a project to build , how can i use a new method, new library, new knowledge, new way to complete this project, automating something in the project , making it less complex etc.\n\nLearning compunds (grows exponentially) over time.\nSkills &gt; built on skills &gt; built on skills…makes you an expert.\nDeliberate practice :\n\n10,000 hours of Deliberate practice makes you Expert.\nDeliberate practice involves Continuously taking effective feedbacks.\nExample :\n\nLearn a Lesson from Fastai and take a Kaggle competition based on the lesson. It will give you feedback on learning.\nBuilding something from scratch & putting it out in world and taking effective feedbacks from people.\n\n\nStay Engaged\n\nDon’t focus on the perfect learning thing.\nFocus on things that keeps you engaged.\n\nCreativity, Work-ethics, Problem-Solving Creation\n\nInspiration is for amateurs — the rest of us just show up and get to work. And the belief that things will grow out of the activity itself and that you will — through work — bump into other possibilities and kick open other doors that you would never have dreamt of if you were just sitting around looking for a great ‘art idea.’ And the belief that process, in a sense, is liberating and that you don’t have to reinvent the wheel every day. Today, you know what you’ll do, you could be doing what you were doing yesterday, and tomorrow you are gonna do what you did today, and at least for a certain period of time you can just work. If you hang in there, you will get somewhere.\nProblem Solving vs Problem Creation : I think our whole society is much too problem-solving oriented. It is far more interesting to participate in ‘problem creation’ … You know, ask yourself an interesting enough question and your attempt to find a tailor-made solution to that question will push you to a place where, pretty soon, you’ll find yourself all by your lonesome — which I think is a more interesting place to be.\n\nIs spaced repetition useful for technical concepts/skills ?\n\nInstead; Try to apply newly learnt concept/skill everday.\nTest it, apply it, discuss it , teach it, write blogs on it etc.\nIf doubt in using it, look up into docs.\n\n\n\n\nYou don’t need a Masters/PHD to succeed at deep learning\n\nMany important breakthroughs are made in research and industry by folks without a PhD, such as the paper “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”—one of the most influential papers of the last decade, with over 5,000 citations — which was written by Alec Radford (a CS undergrad).\nWhat you will need to do to succeed, however, is to apply what you learn in this Fastai course & book to a personal project, and always persevere."
  },
  {
    "objectID": "posts/fastai-guide/index.html#zach-muellers-advice",
    "href": "posts/fastai-guide/index.html#zach-muellers-advice",
    "title": "How to study fastai",
    "section": "Zach Mueller’s advice",
    "text": "Zach Mueller’s advice\n\nHow to get most out of Fastai courses – Zach Mueller\nApproaching each lesson slowly, and letting myself wander in the related concepts, learning as much as I could through online communities.\nWhat is CV? There’s this cool new model out, how would I use it in fastai? My first ever question on the fastai forums was even about debugging a model. But the key here is I didn’t rush things.\nI see far too often people give up and ask for help a day or so into a problem. When I started out, I didn’t ask for help about a week into it. And that’s because I was actively Googling the problem and any relation I could find on it. Not just limiting it to a fastai issue.\nEveryone wants to get there the fastest. And while yes you can, this is often met with others simply handing out the answers, and you not really “learning”. And what winds up happening is the same question gets asked again and again, because they didn’t take a step back and Explore the problem fully and find the root of the issue, or learn a concept that directly relates to it. I purposefully do not do this when someone asks for help, as it can hinder them long-term.\nThrough attacking these problems and learning the fastai framework, I learned how to debug Python, how to work with PyTorch a little bit, and new concepts I had seen for the first time.\nTake it slow, at your own pace. Debug things yourself and get good at standing on your own two feet. At a job, that’s what your life will be like.\nThis is simply what helped me when I was starting:\n\nTaking it seriously, from the start\nComing in with a curious mind\nDriven to see it through\n\nGo through the lectures and courses slowly, relistening and running the notebooks often .\nEssentially go through the notebooks and explore why what is doing what, play around with problem sets, and keep looking back at the original notebooks until it becomes fluid.\n\n\nHow to contribute to Fastai library\n\nIf you have absolutely zero, I recommend watching the first few lectures in the latest part 2 (yes of a few years ago!). Since it’s building a library from scratch, Jeremy covers how he approaches software design, which can help you understand the design https://youtube.com/watch?v=4u8FxNEDUeg&list=PLk25yLtQLOdzmwWy-gZx1kcPDX72NoELg\nFrom there, start small. Look at the simpler areas of the library, and try and add some documentation, etc. Then slowly build yourself towards not understanding how fastai works, but how the DESIGN process works. Apply yourself towards seeing how classes interact, etc.\nA little toying goes a long way. After getting comfortable in some specific area, try writing some newer functions, or perhaps try to fix a bug that got noticed. It will be challenging, and you probably will struggle a bit. It’s not easy and that’s okay. Why?\nWith the discord, forums, and twitter combined, you can find someone (or many someones) to help you understand how to get from A -&gt; B, and that’s what’s wonderful about this community!\nTo take that further, if the part 2 videos are still too high a level, try finding some educational tutorials and courses on Object Oriented Programming, and try and bring up your foundations (pun intended). You can stay in Python for those, so don’t listen to nay-sayers*\n*Other languages are important, and there is a strong belief in starting with C++ or Java as your first language, but I usually reserve that for if you’re actually a CS major or don’t know where to start (which you are Social Sciences learning ML, so that’s okay!)\nThat’s my take on it from my own perspective and belief, I hope some others might be able to chime in on this! HTH :)"
  },
  {
    "objectID": "private/mle-guide/index.html",
    "href": "private/mle-guide/index.html",
    "title": "The Cracked MLE Guide",
    "section": "",
    "text": "The magic you are looking for ; is in just doing things.\n\nYou gain so much more data by doing the thing than by thinking about doing the thing.\nAs Fast.ai taught us - Advanced ideas can be extremely simple and effective when put in practice.\n\nThe magic you are looking for ; is in the work you are avoiding.\n\nYou sit and wonder why you aren’t seeing success.\nWhilst there is a pile of difficult tasks that you are avoiding that will push you closer to your goals.\n\nMotivation isn’t the key — action is. You need action to get motivated, not the other way around. Sitting around and waiting for something to happen never got me.\nIn general, You should focus on systems (what you are doing to get the job) rather than goals (getting the job). The former is within your control, the latter is not.\n\nJust Finish what you started. Finish the Fastai courses + Andrej’s NNs zero to hero + build projects. This is in your control. This will get you a job (not sitting and overthinking.)\n\nThings I learnt by teaching others ~ (Umar Jamil)\n\nMost people struggle to learn not because they lack intellectual capacity. But because their inner voice keeps telling them they’re not smart enough.\nHow to shut the inner voice : Work on Hard things and build confidence. No other way.\n\nDepth-First Procrastination\n\nYou start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die.\n\n\n\n\nAction Brings Results !\n\nAll the successful people across all domains have a common trait.\nThey act, act a lot, iterate quickly and don’t really care what others think."
  },
  {
    "objectID": "private/mle-guide/index.html#motivation",
    "href": "private/mle-guide/index.html#motivation",
    "title": "The Cracked MLE Guide",
    "section": "",
    "text": "The magic you are looking for ; is in just doing things.\n\nYou gain so much more data by doing the thing than by thinking about doing the thing.\nAs Fast.ai taught us - Advanced ideas can be extremely simple and effective when put in practice.\n\nThe magic you are looking for ; is in the work you are avoiding.\n\nYou sit and wonder why you aren’t seeing success.\nWhilst there is a pile of difficult tasks that you are avoiding that will push you closer to your goals.\n\nMotivation isn’t the key — action is. You need action to get motivated, not the other way around. Sitting around and waiting for something to happen never got me.\nIn general, You should focus on systems (what you are doing to get the job) rather than goals (getting the job). The former is within your control, the latter is not.\n\nJust Finish what you started. Finish the Fastai courses + Andrej’s NNs zero to hero + build projects. This is in your control. This will get you a job (not sitting and overthinking.)\n\nThings I learnt by teaching others ~ (Umar Jamil)\n\nMost people struggle to learn not because they lack intellectual capacity. But because their inner voice keeps telling them they’re not smart enough.\nHow to shut the inner voice : Work on Hard things and build confidence. No other way.\n\nDepth-First Procrastination\n\nYou start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die.\n\n\n\n\nAction Brings Results !\n\nAll the successful people across all domains have a common trait.\nThey act, act a lot, iterate quickly and don’t really care what others think."
  },
  {
    "objectID": "private/mle-guide/index.html#andrej-karpathy",
    "href": "private/mle-guide/index.html#andrej-karpathy",
    "title": "The Cracked MLE Guide",
    "section": "Andrej Karpathy",
    "text": "Andrej Karpathy\n\nWhat worked well for me is just :\n\nwriting a ton of stuff from scratch,\navoiding existing libraries as much as I could,\nreproducing other people’s results (eg: in papers), and\nworking on my own random passion projects that use the technology(AI).\nAlso fun to hangout in discords : eg - Huggingface, EluetherAI, GPUMode, Fastai etc.\n\n\n\nHow to become expert at thing :\n\nIteratively take on concrete projects and accomplish them depth wise, learning “on demand” ( i.e. don’t learn bottom up breadth wise)\nTeach/summarize everything you learn in your own words.\nOnly compare yourself to younger you, never to others.\n\n\n\n10,000 Hrs rule\n\nBeginners are always focused on - “what to do?”\nThe Focus should be more on “How much you do”.\nJust pick the things where you can spend time and you’re interested in.\nYou literally have to put in 10,000 hrs of work.\nIt doesn’t even matter as much where you put it.\nYou’ll iterate , You’ll improve and you’ll waste sometime.\nIf you devote yourself to anything diligently for ten years, that will make you an expert.\n\n\nSpend 10,000 hrs on deliberate effort and work, form a daily habit, and compare yourself to yourself from a year ago to track progress.\n\n\n\nKeep Hacking !\n\nSnowball your Projects\n\nKeep on building projects. Just keep the momentum going…\nSome of those projects may end upto nothing, But some of those projects will snowball into bigger and interesting projects.\nExample: Andrej started with game programming tutorials → Started to love teaching → taught CS231n → Neural Nets zero to hero → LLM101n course\nAt OpenAI , a reddit chatbot project → when transformers came out → became GPT 1,2,3,4 …\n\n\n\n\nPut in the time (10,000 Hrs)\n\nDon’t be nervous about - What am i working on ; Am i succeeding or failing ?\nJust count “how many hours you’re doing & everything adds up.\nEven the projects that fails / doesn’t snowball.. Those count to no. of hours I will spend developing my expertise.\n( 8 hours a day) x (4 years) = 10,000+ hours\nExperience (10000+ hrs) makes you (World class) Expert !\n\nKeep the dopamine flowing\n\nYour brain is a reward machine. It needs rewards.\nKeep the dopamine flowing → Work on projects. Publish your stuff.\nProjects get you to work on something end to end depthwise.\nIn class, you learn it breadthwise, learning a lot of stuff , just in case you need it in future\nWhen you’re working on a project - You know what you need and you’re learning it on demand and you’re just trying to get it to work!\nPublishing it online , makes you think - how to make it really good.\n\n\n\n\nHow to become a cracked ML Engineer\nThe most concise piece of advice I can give you:\n\nFind a project you care about\nLearn stuff “on-demand” that will take you to the completion of the project\nAnnounce your project to the world (GitHub, social) - build those public creds\nRepeat 1. - increase the scale / ambition. Rinse and repeat\n\nExample : find a project that requires efficiency and that would, without an efficient implementation, annoy the hell out of you. Loading that long array of images is ultra slow? Go and learn, on-demand, better algorithms and solve your problem.\n\nYou can just do things.\nIgnore the gate-keepers.\n\n\n\nOn Shortification of Learning & “How to actually Learn”\n\nLearning is not supposed to be fun. It doesn’t have to be actively not fun either, but the primary feeling should be that of effort.\nIt should look a lot less like that “10 minute full body” workout and a lot more like a serious session at the gym. You want the mental equivalent of sweating.\nSo for those who actually want to learn. Unless you are trying to learn something narrow and specific, close those tabs with quick blog posts. Close those tabs of “Learn XYZ in 10 minutes”.\nConsider the opportunity cost of snacking and seek the meal - the textbooks, docs, papers, manuals, longform. Allocate a 4 hour window. Don’t just read, take notes, re-read, re-phrase, process, manipulate, learn."
  },
  {
    "objectID": "private/mle-guide/index.html#yoobin-ray",
    "href": "private/mle-guide/index.html#yoobin-ray",
    "title": "The Cracked MLE Guide",
    "section": "Yoobin Ray",
    "text": "Yoobin Ray\n\nLearning Resources for “Cracked” DL Practitioners\n\nFoundational SWE\n\nClean Code\nRefactoring Book\nPhilosophy of Software Designs\nDesign Patterns\nClean Architecture\nAdvanced System Design Papers\n\nFoundational ML\n\nFocus on implementing Key models from Scratch\n\nLinear Regression\nMLP networks\nPolynomial Regression\nKNN\nK-means Clustering\nParameter fitting via GD\nDecision Trees\n\nPickup a good ML Textbook & Read it (whenever you have time)\n\n100 page ML Book - Andriy Burkov\n[[ML with Pytorch and Scikit-learn - Sebastian Raschka]]\n\nSome Maths\n\nIntro to Algo and ML - Mathacademy\n\n\nFoundational DL\n\nFinish DL Courses\n\nFastai Book + Courses\nNeural Nets 3b1b\nPytorch\nBuilding Micrograd - Karpathy\nCS231n stanford\nFull Stack DL\n\nBuild from Scratch\nCompete in Kaggle\nNLP Foundations + Cutting Edge Stuff\n\n[[NLP with Transformers]]\nFastai Part 2 Course\nHugging Face Diffusion Course\nKarpathy’s Playlist\nUmar Jamil’s playlist\nBuild Llama3 from Scratch\nImplement Papers\n\nHardware Optimization -&gt; “for Cracked”\n\nJeremy Howard’s Cuda lectures\nCUDA MODE Lectures, Discord\nC, C++ (Tutorials)\n\n\nFoundational CS\n\nLeetcode is a MUST - [[DSA Sheet - leetcode]]\n\n\n\n\nHow to Self Study ML without wasting time\n\n\nThe ML Researcher path\n\nlearn the intuition of classic trade offs in statistics like bias/variance, what happens when your dataset is noisy, etc.\nLearn ML theory using any good book/course\nlearn the early DL works before reading Attention is All you Need\nCNNs, NLP, LLMs, IT DOESN’T MATTER, choose whatever is cool to you in a given moment and just understand papers from a intuition perspective like “ah I see why they would add a final sigmoid activation at this step since we’re working with probabilities for multiple classes” or “ah I see why they actually need attention here because vanilla RNNs would just lose that context in its hidden states” etc. -&gt; repeat with X amount of topics within DL\nthe trick here: GO DEEP. Intuition. Intuition. Intuition. This is what I lack when reading research papers lately cuz I’m so optimized for engineering but if you wanna be a rock start researcher get that INTUITION!\n\n\n\nThe ML Engineer path\n\nRead Pytorch docs, a book, or a youtube tutorial on it\nWatch ANY build from scratch video\nRead and Implement papers from scratch untill you can do it without much help\ndissect how big real world companies are designing their ML systems (blog posts usually)\nthe trick here: GO WIDE. Get good at multiple aspects of ML engineering—date engineering with spark, building models, fine tuning models, design CI/CD pipelines with the data orchestration (idk use airflow or stepfunction or something), understand the deployment process in production (MLOPS), dive into scalable architectures from Pinterest, Google, META, etc.\n\n\n\nThe Deep Work\n\neat, sleep, read research papers, reimplement paper baselines and novel models in PyTorch, repeat.\nI fought procrastination by not giving myself any other options – I literally locked myself in the library study room without my phone many nights or I would deliberately go to a different campus without my phone.\n\n\n\nDifferent types of MLEs\n\nThe different types of MLEs\n\n\n\nDon’t waste time like I did in my ML career\n\nDon’t waste time like I did in my ML career"
  },
  {
    "objectID": "private/mle-guide/index.html#andrew-ng",
    "href": "private/mle-guide/index.html#andrew-ng",
    "title": "The Cracked MLE Guide",
    "section": "Andrew Ng",
    "text": "Andrew Ng\n\nDevoted to Reading and Learning\n\n“In my own life, I found that whenever I wasn’t sure what to do next, I would go and learn a lot, read a lot, talk to experts. I don’t know how the human brain works but it’s almost magical: when you read enough or talk to enough experts, when you have enough inputs, new ideas start appearing. This seems to happen for a lot of people that I know.”\n“When you become sufficiently expert in the state of the art, you stop picking ideas at random. You are thoughtful in how to select ideas, and how to combine ideas. You are thoughtful about when you should be generating many ideas versus pruning down ideas.”\nAndrew Ng thinks innovation and creativity can be learned — that they are pattern-recognition and combinatorial creativity exercises which can be performed by an intelligent and devoted practitioner with the right approach.\n\n\n\nDebugging ML Faster\n\nDon’t spend 6 months trying to pursue a particular direction such as Collect More Data ; Sometimes you could run some tests, could have figured out 6 months earlier ,that collecting more data isn’t gonna cut it.\nPeople that are good at really debugging ML algorithms are easily 10-100x faster at getting something to work.\nThe question is - Why doesn’t it work yet ?\nTry to modify/change the architecture , More data, more regularization, different optimization algorithms , different types of data,… So to answer those those questions systematically.\nso you don’t spend 6 months heading down a blind Alley..\n\n\n\nConsistency beats Intensity (Always)\n\nReading 2 research papers is a nice thing to do.\nBut the power is not reading 2 research papers.\nIts reading 2 research papers per week for 1 year. Then you’ve read 100 papers.\nAnd you actually learn a lot when you read 100 papers.\nThis sustained effort for a really long time - will make you a great DL practitioner. Not the bursts of efforts/all nighters on some days of the year…"
  },
  {
    "objectID": "private/mle-guide/index.html#jeremy-howard",
    "href": "private/mle-guide/index.html#jeremy-howard",
    "title": "The Cracked MLE Guide",
    "section": "Jeremy Howard",
    "text": "Jeremy Howard\n\nTo become a world class DL Practitioner\n\nWilling to learn\nCuriousity\nTenacity\nConsistency (10,000 hrs)\n\n\n\nLearning by Discovery (How to learn DL in Fastai way)\n\n\nIn deep learning, it really helps if you have the motivation to fix your model to get it to do better. That’s when you start learning the relevant theory.\n\nBut you need to have the model in the first place. Learn almost everything through real examples.\nAs we build out those examples, we go deeper and deeper, and we’ll show you how to make your projects better and better.\nThis means that you’ll be gradually learning all the theoretical foundations you need, in context, in such a way that you’ll see why it matters and how it works.\n\n\n\n\nFastai : Lesson 0\n\nSchedule the course - how long will u take to finish each lesson.\n\non which day will u watch the Lesson , what will u do after watching Lesson (next steps) etc. (to avoid procrastination & maintain consistency)\n\nDon’t add too many fillers while doing the course\n\ni need to learn all the numpy and pandas and matplotlib and python before i go to next lesson this will break the flow of your learning fastai\nJust learn enough so that you can search the thing whenever needed.\nif you start learning all the pandas features…it will alone 6 months.\nJust learn fundamentals and carry on training models (dooing DL).\n\nTeaching the whole game\n\nWe’ll start off by showing you how to use a complete, working, usable, state-of-the-art deep learning network to solve real-world problems using simple, expressive tools.\nAnd then we’ll gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on…\n\nThe hardest part of deep learning is artisanal:\n\nHow do you know if you’ve got enough data, whether it is in the right format, if your model is training properly, and, if it’s not, what you should do about it? That is why we believe in learning by doing.\nAs with basic data science skills, with deep learning you get better only through practical experience. Trying to spend too much time on the theory can be counterproductive.\nThe key is to just code and try to solve problems: the theory can come later, when you have context and motivation.\n\nGet started with coding\n\nEven if you are not good at coding -\n\nRather than thinking - it’s a shame “i don’t know coding”\nNow you have a really fun project (fastai) - to learn coding for & from.\n\nLots of people have become great coders while doing the course (Zach Mueller) . You’ll learn lots of CS like -\n\nOOPs, Functional programming\nlist comprehension\ngpu acceleration, software design, etc..\n\n\nDon’t try to go in depth in Fastai library\n\nBecause , If you’ve finshed fastai part 1 , part 2 + live coding + fastbook + walk with fastai  – Then, you’d have implemented all the fastai library (from scratch + all the Deep learning fundamentals.)\n\nUtilize the Clean version of fastbook :\n\nThis notebook contains just code (no context).\nAfter reading the main fastbook , come to this version - see the code and before running the code…\nAsk yourself\n\nwhy is this cell here ?\nwhat’s it for ?\nwhat’s it gonna do ?\nwhat’s the input and output gonna look like ?\n\n\nHow to use the Provided Notebooks\n\nRead through the notebook. If everything makes sense, put it aside and create a new notebook.\nNow try to code the same process as we went through in class.\nIf you get stuck at any point, you can refer to the class notebook. Find the solution to what you are stuck on. Look up the relevant documentation. Put the class notebook aside again, go back to your notebook, and try to code the solution.\nIf you are still stuck, you can refer to the class notebook again. Do not copy and paste the needed code. Instead, type it out yourself. Check that it runs. If so, try changing the inputs, and see if that effects the outputs as you expect.\nAny time that you feel unsure about why a particular step is being done, or how it works, or why the outputs are what you observe (or anything else), please ask on the forums. As I write this (week 3 of the course), there has not been a single question on the forums that has not been resolved!\n\nThere will be times when the journey feels hard.\n\nTimes when you feel stuck. Don’t give up!\nRewind through the book to find the last bit where you definitely weren’t stuck, and then read slowly through from there to find the first thing that isn’t clear.\nThen try some code experiments yourself, and Google around for more tutorials on whatever the issue you’re stuck with is—often you’ll find a different angle on the material that might help it to click.\n\nAlso, it’s expected and normal to not understand everything (especially the code) on first reading.\n\n Trying to understand the material serially before proceeding can sometimes be hard. Sometimes things click into place after you get more context from parts down the road, from having a bigger picture. So if you do get stuck on a section, try moving on anyway and make a note to come back to it later.\n\n\n\n\nYour Projects and Mindset\n\nadd these contents under one “Projects” section!\n\n\n\nTenacity and Deep Learning\n\nThe story of deep learning is one of tenacity and grit by a handful of dedicated researchers. After early hopes (and hype!), neural networks went out of favor in the 1990s and 2000s, and just a handful of researchers kept trying to make them work well. Three of them, Yann Lecun, Yoshua Bengio, and Geoffrey Hinton, were awarded the highest honor in computer science, the Turing Award (generally considered the “Nobel Prize of computer science”), in 2018 after triumphing despite the deep skepticism and disinterest of the wider machine learning and statistics community.\nHinton has told of how academic papers showing dramatically better results than anything previously published would be rejected by top journals and conferences, just because they used a neural network. Lecun’s work on convolutional neural networks, which we will study in the next section, showed that these models could read hand‐ written text—something that had never been achieved before. However, his break‐ through was ignored by most researchers, even as it was used commercially to read 10% of the checks in the US!\nIn addition to these three Turing Award winners, many other researchers have battled to get us to where we are today. For instance, Jurgen Schmidhuber (who many believe should have shared in the Turing Award) pioneered many important ideas, including working with his student Sepp Hochreiter on the long short-term memory (LSTM) architecture (widely used for speech recognition and other text modeling tasks, and used in the IMDb example in Chapter 1). Perhaps most important of all, Paul Werbos in 1974 invented backpropagation for neural networks, the technique shown in this chapter and used universally for training neural networks (Werbos 1994). His devel‐ opment was almost entirely ignored for decades, but today it is considered the most important foundation of modern AI.\nThere is a lesson here for all of us! On your deep learning journey, you will face many obstacles, both technical and (even more difficult) posed by people around you who don’t believe you’ll be successful. There’s one guaranteed way to fail, and that’s to stop trying. We’ve seen that the only consistent trait among every fast.ai student who’s gone on to be a world-class practitioner is that they are all very tenacious.\n\n\n\nBeing tenacious also means …\n\nKeep going (until u finish the course , project , competition)\nKeep coming back & continue the Fastai Lessons (even if u took a break of 2-3 days or 2-3 years)\n\n\n\n\nSelf-Learning Tips\n\nLearn on demand\n\nIf you find you need to know some foundational skill, learn it at that time, and just what you need. A lot of students get lost in rabbit holes of foundational math study and never develop expertise in AI/ML as a result!\n\nDoing an Actual Project by using something new.\n\nI have a project to build , how can i use a new method, new library, new knowledge, new way to complete this project, automating something in the project , making it less complex etc.\n\nLearning compunds (grows exponentially) over time.\nSkills &gt; built on skills &gt; built on skills…makes you an expert.\nDeliberate practice :\n\n10,000 hours of Deliberate practice makes you Expert.\nDeliberate practice involves Continuously taking effective feedbacks.\nExample :\n\nLearn a Lesson from Fastai and take a Kaggle competition based on the lesson. It will give you feedback on learning.\nBuilding something from scratch & putting it out in world and taking effective feedbacks from people.\n\n\nStay Engaged\n\nDon’t focus on the perfect learning thing.\nFocus on things that keeps you engaged.\n\nCreativity, Work-ethics, Problem-Solving Creation\n\nInspiration is for amateurs — the rest of us just show up and get to work. And the belief that things will grow out of the activity itself and that you will — through work — bump into other possibilities and kick open other doors that you would never have dreamt of if you were just sitting around looking for a great ‘art idea.’ And the belief that process, in a sense, is liberating and that you don’t have to reinvent the wheel every day. Today, you know what you’ll do, you could be doing what you were doing yesterday, and tomorrow you are gonna do what you did today, and at least for a certain period of time you can just work. If you hang in there, you will get somewhere.\nProblem Solving vs Problem Creation : I think our whole society is much too problem-solving oriented. It is far more interesting to participate in ‘problem creation’ … You know, ask yourself an interesting enough question and your attempt to find a tailor-made solution to that question will push you to a place where, pretty soon, you’ll find yourself all by your lonesome — which I think is a more interesting place to be.\n\nIs spaced repetition useful for technical concepts/skills ?\n\nInstead; Try to apply newly learnt concept/skill everday.\nTest it, apply it, discuss it , teach it, write blogs on it etc.\nIf doubt in using it, look up into docs.\n\n\n\n\nYou don’t need a Masters/PHD to succeed at deep learning\n\nMany important breakthroughs are made in research and industry by folks without a PhD, such as the paper “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks”—one of the most influential papers of the last decade, with over 5,000 citations — which was written by Alec Radford (a CS undergrad).\nWhat you will need to do to succeed, however, is to apply what you learn in this Fastai course & book to a personal project, and always persevere."
  },
  {
    "objectID": "private/mle-guide/index.html#radek-osmulski",
    "href": "private/mle-guide/index.html#radek-osmulski",
    "title": "The Cracked MLE Guide",
    "section": "Radek Osmulski",
    "text": "Radek Osmulski\n\nHow to become mini-Edison in your field of interest?\n\nFollow your curiosity\n\nI have no clue why this works, but it does. I keep seeing this over and over again.\nMaybe we are naturally inclined to search for things that can be effective?\n\nPractice over theory.\n\nThere exist two parallel worlds – a world where you are rewarded for being knowledgeable and a world where you are rewarded for making things that work.\nIt’s interesting to see GE Research attempting to leverage both.\n\nGenerally, there is no overlap.\n\nPeople tend to maximize one or the other which produces really mind-twisting situations.\nIn 99.999% of technologies informed tinkering precedes theory.\nSo you have to choose which world to make your home.\nOr maybe the decision is made by who you are and your personal history?\n\nBuild many small projects to get a good feel for what the technology can do\n\nSeems that Edison knew a lot of practical things and a large part of this understanding came from failed experiments.\nIf you want to have a good feel for what might work, you have to build and experiment relentlessly.\nThat is very interesting, as this sounds like project-based learning from the @fastdotai course? 🤔\n\n\n\n\nThe most important skills for Machine Learning:\n\nPython\nLinux (env setup, ssh, moving files, editor)\ngit\npdb\nCreating good train - val - test splits\nAbility to scan papers for relevant information\nA learner mindset\nClear writing\nBasic statistics\nExperiment design\n\n\n\nRadek’s DL journey\n\nI would suspend my disbelief and do exactly what Jeremy Howard tells me to do in the fastai lectures.\nFinish Fastai & follow the instructions precisely.\nI will train a machine learning model after machine learning model.\nStart your Kaggle Journey Sincerely.\n\nfigure out how to move the data around and efficiently feed it to my deep learning model!\nlearn how to set up a remote VM for work and how to navigate around Linux.\n\nBecome extremely active on Fastai forum & Discord , Kaggle Forum.\n\n\n\nPersistency is everything\n\nJeremy Howard was asked about what seperates the students that do really well for themselves from students who don’t. Without hesitation he answered - “Perseverence.”\nI am in my 8th year of this journey, I won a Kaggle competition and I am writing a book about machine learning. But most of the time I feel like a beginner. The code I write has bugs. I execute on the tactics I describe in this book imperfectly.\nBut I continue to get better and better results. And, I don’t know a person who would be more surprised by this than me!\nI look around at the people I met along the way. They come from various backgrounds, had various starting points. My assumption is that if you were to know them better, or to know me better, you would be surprised by how ordinary we are. When it comes to me, ordinary is not even a good word. Fallible, misguided, caring about things that have no relevance in the longer run. All these would work better to describe me.\nAnd yet here we are, where you are reading the words that I wrote. Where I finally have a job I am very happy about in an industry well known for how well it pays.\nI can’t help to not agree with Jeremy Howard. He met way more people than I have and has a sharper mind. The only apparent distinguishing factor is how persistent one is.\nIf you have a good map of the terrain, you can arrive at interesting outcomes much sooner than I have.\nNonetheless, one thing is certain &gt;&gt; At any given moment, as you put in the work, you can barely notice a difference in your life. But the longer you stay the course, the more rewarding the journey becomes. Learning compounds and you need to give it time to start seeing the exponential results.\nCombine Persistence with community involvement and you cannot be stopped.\n\n\n\nHow to gain a competitive edge over 95% of people learning ML\nMachine learning is a vast ocean of ideas. That is why you need this very important skill to thrive in this sea of wonder :\nAt every point of your Machine Learning journey, you need to be crystal clear about what you are after. - Do you want to break into tech? - Do you want to become better at solving real-life problems? - Do you want to contribute to open source?\nYou can – and should – do many things over a year. But you should work on only a single project at a time.\nSun heats up and feeds the whole planet. A laser needs 1/100000th of the energy to cut through rock. Here are a couple of examples:\nAre you doing the fastdotai course? That means you are not: - taking MOOCs on @coursera - becoming a DALL-E prompt engineer - learning web dev from this one cool YT personality\nWant to win at kaggle ?\n\nHere are the things you are not doing then:\n\nreading that fascinating book about doing statistics in APL\n\nlearning about programming paradigms on@edXOnline\ntaking up crocheting (which is super fun, BTW)\n\nThere is no magic to success.\n\nYou do not need any special powers.\nYou pick one thing and you do it every day.\nYou look at your progress over X.\nX ∈ {week, month, year, decade}\nYou ask yourself – is this what I need more of?\n\nIf YES: pass\nelse: do something else\n\n\nYou do not need to do a single thing in life only.\nBut you need to only do a single thing at a time.\n\n\n\nThe Secret to learning ML Faster ⚡:\n\nWhat is the ONE THING that is an obstacle to doing Machine Learning better?\nNot understanding the theory behind some of the models? Not knowing the math?\nBe honest with yourself.\nBased on your experience, based on what you are seeing around you, is this REALLY what is in your way?\nWhat is the shortest path FOR YOUR SPECIFIC CIRCUMSTANCES to massively improve at machine learning?\nYou have to be sensitive to your conditions.\nFor Example : As I started to get really deep into Machine Learning ; I observed that the main thing stopping me from becoming much better at ML was my ability to read and write Python!\n\nThe whole obsession with math and reading papers was just a mirage.\nThe true area where I was hurting was my ability to read and write Python.\nOnce I identified this the improvements were easy and results dramatic.\n\nSo what is the one thing that is in your way of becoming a better Machine Learning practitioner?\nIs it your ability to configure your own workstation or set up a cloud VM?\nIs it your ability to structure an ML project and to grow it organically?\nWhat is it?\nFocus on that one thing. Don’t worry about what others do. You are the main protagonist in your own story!\nDon’t listen to what worked for people who are vastly different to who you are.\nReact to your current setting.\nEliminate the bottleneck to make the whole system of you as a machine learning practitioner running much more smoothly!\n\n\n\nKeep It Simple (KISS) 😇\n\nGive priority to really doing Machine learning / Deep learning , which will keep you motivated to learn all the secondary things around doing ML/DL , like setting up cloud VM , learning the Math needed, software design etc..\nThis way you learn the things just needed to improve in DL ; Instead of spending months of research/learning those secondary things ; which results in Loss of Motivation and productivity and slow the progress.\n“To start, focus on what things DO, not what they ARE”\n\n\n\nUse Constraints to learn Machine Learning 12x faster! 🕐\n\nTraining the models and violating them properly ie. robust evaluation and testing methodologies in assessing the reliability and generalizability of ML models beyond their initial training data.\n\n\nIn ML, after training a model using a dataset, it’s crucial to evaluate its performance on unseen or test data to assess how well it generalizes to new, unseen examples. This evaluation involves various metrics and techniques to measure the model’s accuracy, precision, recall, F1-score, etc., depending on the problem type (classification, regression, etc.). deliberately testing the model’s limits by providing it with challenging or edge-case data points that might not align perfectly with the patterns seen in the training data. This process helps in understanding how the model behaves under different conditions and whether it can handle unexpected scenarios or outliers effectively.\n\n\nUse constraints and Focus on whats important:\n\nInstead of dreaming of for the best DL hardwares , figuring out cloud VMs ; See how far you can get in a kaggle competition just by using Kaggle notebooks.\nfocus on What are the quickest improvements possible to improve your models.\nUse Constraints : Set constraints on yourself ; to clean the space and focus on what matters.\nDon’t keep on researching for weeks:\n\nTimebox 2-4 hours or a day for thorough research; for what framework to use in this situation. And then Stop researching.\nJump into the framework , get good at it.\n\nFind your fav ML expert; and Study the courses/books they used to learn ML.\n\nNo course is perfect.\nThe value is in accumulating the concepts and knowledge and then moving forward.\n\n\n\n\n\nThe Most important component of any ML project 👤\n\nSo what is the most important component of any ML project?\nIs it the CPU, amount of RAM, disk space?\nOr maybe it is all about the GPU that you have? Or maybe the data?\nThere is only one component that is shared across each and every project that you work on. And that is - YOU!\nThe hardware, the data, all of it are just ingredients. And you are the chef.\nAnd therefore it makes absolute sense to devote the time and energy to grow your skills.\nIn fact, traversing the Kaggle forums, I am constantly reminded how much people can achieve with very little!\nInstead of spending a month researching the tiny differences between various GPUs Invest that time into actually training models AND developing your skills?\nAnd if you are struggling with time management, you are not alone.\nBut you can always make your life simpler by deciding what you will NOT give your time, energy and attention to.\nNot only not getting distracted by Netflix, but also not getting distracted by another free course that we could passively consume.\n\n\n\nYou have to get out of tutorial hell as soon as you can\n\nYou have to get out of tutorial hell as soon as you can. Its a complete waste of time.\n\nHow @fastdotai courses avoid this problem\nWhat i am doing to not get stuck in tutorial hell when learning from other sources.\n@fastdotai courses were designed using the top-down philosophy.\nThe Idea:\n\nShare enough in a lecture to get one up and running.\nEach lecture was a complete project – it provided all you needed to train an image classifier, recommeneder model, etc.\nAlso each lecture featured a call to action!\n\nWalk through the notebook & figure out all pieces work together.\nTrain the model on a different dataset.\n\n\n\nThis is what i am doing to address this in my life that you can copy:\n\nReflect on how much time you wasted due to tutorial hell in the past.\n\nExperience on its own is useless.\nOnly when you reflect on the experience and re-live the futility of an approach you tried in the past can you change your ways.\n\nCut following a tutorial short.\n\nMost non-@fastdotai tutorials attempt to teach something complex.\nBut usually, you need very little to get going.\nStop early and work on something of your own.\nScan the rest of the tutorial to know where to look for info when needed.\n\nConvert learning(going through a tutorial) to a standalone project.\nExamples of such projects:\n\n@Kaggle competitions\na repository you create and tweet about\na blogpost explaing said thing\n\n\nThere are two crucial reasons why you should start working on a project of your own as soon as you can:\n\nIt is very motivating.\nYou never know what you will need to learn up front.\n\n\nThere are so many things one could learn on any subject!\nLearning as you go – or just-in-time learning, is an incredible hack.\nIt tells you exactly what you need to learn.\nVery often, you will be surprised at what you are missing!\nYou move fast not by hustling/dumping hours into doing stuff.\nYou move fast by honing your ability to focus on the essentials.\nTwo vastly different approaches.\n\n\n\nLife changes , OR how the Fastai course went from being too high level to too low level from my perspective\nI remember taking the fastai course for the first time.\n“OMG this is so high level, how is this even learning? Where are the derivatives?”\nThat was at a time when I studied machine learning.\nI didn’t compete at Kaggle. I didn’t have an ML job. I watched lectures, read books and papers, pondered set theory and the construction of real numbers, I didn’t even realize that PyTorch (or anything else with autograd) existed and I wrote code like this.\nFast forward 5 years to this week’s lecture. Having worked in ML for the last 4 years or so.\n“OMG this is so low level. Ahhh so this is how we calculate the bias term, and this is where we stick with torch.no_grad(). Ahhh, I see.” I found this experience quite surprising What is the meaning of all this?\nOn one hand, it might be that what you do in the workplace, how you bring value to the world, is very far removed from the perception of learning instilled in us at school.\nThe acquisition of skills that make us valuable in the real world might not feel like learning at all!\nCase in point – the job that I started just recently. I am getting so much value in my work from - being able to identify what is important to focus on (we practice this as we learn a new way of reading papers which will most likely be discussed later in the Fastai course) - how to learn from jupyter notebooks created by others (this literally mirrors the process of doing the fastai homework!) - being able to set up my own work environment, install software libraries, navigate to parts of the codebase that are relevant without giving it too much thought, all of this has become second nature to me and is something that we practice in this course\nAnd the list above doesn’t even mention probably the biggest component of what I do on a regular basis, that probably led to me having ML jobs in the first place, and that is technical writing! Something that felt like play, that I only continued to practice because of fastai, but that actually turns out to be the real work despite seeming to the contrary! (a great blog post on how to get started 8)\nAnother lesson I derive from this experience is that your perception of what is covered in the course speaks more about you and where you are in life than about the course itself.\nThe best thing one can do for oneself is to approach the course with an open mind. Follow the instructions and withhold your disbelief for as long as you can.\nFrom that it springs, that the best way to evaluate this course is not by how you feel about it now but by the effects it produces for the ones who have taken the course some time ago.\nAnd the effects are quite good :\n\n\n\n\n\nThe Secret Power of Doing Things Badly\n\n“Doing things badly” is a metaphor for overcoming perfectionism and fear of failure. Instead of paralyzing oneself with the need to do everything perfectly, embracing imperfection allows for action, experimentation, and ultimately, progress.\nPerfectionism is a roadblock to progress. The fear of failure can prevent us from even starting, while striving for impossible ideals can lead to frustration and burnout.\nSmall, imperfect actions are often more powerful than grand plans never acted upon. By taking tentative steps and accepting stumbles along the way, we pave the path for learning and growth.\nA playful and positive attitude can unlock creativity and open doors to opportunities. Approaching challenges with humor and curiosity can lead to unexpected solutions and breakthroughs.\nBuilding connections and seeking support from others is crucial for navigating life’s challenges.\nPractice saying “yes” to new experiences, even if they feel outside your comfort zone. What opportunities have you been avoiding out of fear? How can you step outside your comfort zone and embrace the unknown?\n\n\n\nThe problem with NOT GOOD ENOUGH\n\nDo you know how it feels to start writing a blog post but never publish it?\nTo tweet but delete it 5 minutes later?\nTo start working on a GitHub repo but never share it “cause the code is not clean enough”?\nThese are all variations on the same theme.\nNot accepting that you are human. That you are vulnerable and fallible.\nBy not telling the complete story of who you are you deeply hurt yourself.\nYes, deleting a tweet after publishing it is 100x better than not tweeting it in the first place.\nBut this behavior has its roots in a sense of shame. Along with elevated, unattainable expectations.\nUNATTAINABLE to you where you are right now in your journey.\nListen, if I could record a better video right now, I would. If you could write a better blog post, you would.\nBut by not embracing who you are right now you limit your opportunities to learn and to grow.\nOur mind evolved to protect us from harm. This is the main reason why we feel so uncomfortable in new situations.\nBut if we put ourselves in those uncomfortable spots and give our mind the chance to observe that actually, nothing bad happens to us, and we do this repeatedly… &lt;- This is how we learn\nYou do not become better by sitting on the bench. You become better by playing the game.\nGive yourself the chance. Go out onto the field and swing that bat even if it feels that you stand no chance.\nFor first 100 swings you do not. But on the 101 swing something magical is likely to happen.\n\n\n\n\nThe Secret to Learning Anything Well\n\nLearning has two phases:\n\n✅ first – you have to become comfortable doing something badly\n✅ only then – you improve\n\nHere is what happens when you skip the first phase.\n\nAre you learning machine learning?\nYou might be feeling lost.\nYou might have a hard time figuring out what to work on.\nYou might think to yourself:\nI will get better if I learn more math.\nbzzzzzzzzzz\nWrong answer.\n\nWhy?\n\nDid you complete the first phase?\nDid you learn to DO machine learning badly?\nDo you feel comfortable moving data around?\nDo you feel comfortable applying basic functionality from libraries such as @fastdotai or scikit to your data?\n\nHere is why this is important.\n\nIf you are learning ML and are not doing ML badly…\nChances are you are not doing any ML at all!\nYou are dying to perfectionism.\nThis leaves you flying blind.\n\nHere is what I mean.\n\nYou cannot learn to write poetry before you learn the alphabet.\nNo one can.\nThere is nothing wrong with you.\nMachine learning is not harder than anything else.\nYou are just taking the wrong approach.\n\nWhat will happen if you learn to do machine learning badly first?\n\nYou will gain an understanding of the end-to-end process.\nFor every action that you will take you will receive feedback.\n\n\nThis is HUGE! ☝️\nMachine learning will become very real to you. You will start building an intuition how to move around in it.\nMore importantly\n\nNow that you’ve made machine learning tangible.\n\nYou can let it guide you in your learning!\nYou no longer have to guess What to study!\nThrough feedback, you will know precisely What you need to improve.\nThis makes a world of a difference.\n\nIts the difference between walking around blindly , and moving directly towards your destination.\nYou can learn to do anything well\nBut only if you first learn to be comfortable doing it badly.\n\n\n\nA couple of things in my life I want to do more badly\n\n\nKaggle\nTalking online\nFollowing my curiosity\nBeing more focused\n\n\n\nFirst of all, when you try to do something well , It often takes such a toll on you that you don’t do it at all.\nThings that could be fun become less fun.\nThere is a lot of stress that comes with trying to do things well. Who needs more stress?!\nHow can you benefit from the Pareto Principle if you keep doing things well? D’oh\n\n\n\n\nLearning is a product of quantity\n\nhow many ML models you train ?\nhow many blog posts you write ?\nhow many issues you debug ?\n“Produce indiscriminately, consume selectively”\n\n\n\nQuantity leads to Quality\n\nIt took me 8 years to learn this truth about learning ML.\nI’ll teach it to you in the next 5 minutes:\nI used to think\n\nIf I complete this one MOOC\nIf I write 8 blog posts in 8 weeks\nIf I do well in a @kaggle competition\n\nI will\n\nhave learned ML\nmy life will change in a substantial way\n\nThat didn’t reflect reality.\nWhen your mental map does not align with the world\n\nyou experience stress\nyou are less effective than you could be\nyou lose the ability to direct your life\n\n\nThe proper mental map is this:\n\nThe only thing that matters is doing something over months or years.\n\nIf you believe\n\nkaggle\nblogging\ndoing MOOCs\nsomething else\nis your path, your only goal is to build it into your life.\n\nTo do it consistently over a long while.\nYour short-term results do not matter.\n\nWhether you feel you are learning or not doesn’t matter.\nAll that matters is consistency.\n\nThere are no exciting destinations you can get to in life in 24 hours Or a month Or a year.\nDon’t write a single blog post. Write a hundred.\nDon’t participate in a single @kaggle competition. Keep competing for the next two years.\n\nChange your perspective to change your world.\n\n\n\nThe Secret to Becoming a Data Scientist\n\nPerseverance is Not Intensity\n\n\nwhile intensity can offer quick progress and immediate results, it may sometimes hinder long-term retention, adaptability, and sustainable progress.\nPerseverance, on the other hand, promotes a more balanced and sustainable approach to learning, allowing for deeper understanding, adaptability, and long-term retention of knowledge and skills.\n\n\nPerseverance is Not Discipline\n\nAvoid HARDWORK and DISCIPLINE to work like a labour.\nEnjoyment should be the reason to work, not the results.\nFind interesting ML Projects to build + Give Kaggle Competitions + Learn ML in community … etc\n\nPerseverence is Finding something which I find exciting and thus keep on doing it for a long time (more than 2 decades)\nEnjoyment is the key to Excellence (in every aspect of life).\nWhy Perseverance works so well in learning ML ?\n\nPerseverance helps to build Strong Neural Connections of Knowledge.\nEntering into a new field (ML/DL/LLM…etc) ; You have to keep going , and find strategies that work. Embracing failures and try again try again.\n\n\n\n\nJourney of becoming employable in Deep Learning\n\nWhat I thought becoming employable in deep learning would be\n\nlearning a lot of math\n\nWhat it ended up being:\n\nlearning how to talk about your work\nbeing able to point to the things you’ve done\nlearning how to learn\nbeing open to new experiences\nhelping others\nlearning the craft of delivering good ML solutions to real-life scenarios (not covered by most textbooks/courses)\nfiguring out how to use social media and limiting the negative footprint\n\nThe Imposter syndrome\n\nwhen you think of yourself from 6 months, 12 months ago, can you see the progress you have made?\nbeing able to monitor one’s progress is super important.\nBut it might not be best to compare oneself to others on social media. The environment favors hype and unsubstantiated claims and genuine progress and results take time. Many people chose the easier route because it works.\nyou will run into people that share with honesty, but they might have been doing something for many years now and it is not apparent. I know from my own experience that the speed I was able to move at in my 2nd year of studying machine learning was dramatically different from that in my 6th year, but looking outside in no one would probably be able to tell how much I have changed.\nSometimes the years of experience do not matter that much as I would suggest above - it’s more about what ideas and techniques you encounter at what point in time.\n\n\n\n\nDo tell the world - What you are upto !\n\nif you do something, even with a modicum of success, make sure to tell the world about it!\nIt DOESN’T MATTER how good your writing is! Just type those characters out onto a screen and send them into the world.\n\n\n\nWhat to do once people know you through your work ?\n\nIf you go deeper on something in a project, share something that might be useful to users of popular libraries, or put together a howto that can help a community… chances are you might appear as a blip on someone’s radar. Someone who might be an “industry insider”.\nif you are interesting enough, some of these people will reach out to you and introduce themselves in a DM! Or you might get to know them at a conference or a meet-up in real life.\nWell, if you are looking for a job, TELL OTHERS that you are looking for a job. This is the crucial ingredient, I cannot overemphasize it enough.\nIf you’ve met this one person who took interest in your projects, and they seem like someone who is well connected in the industry, send them an email asking if they know of any openings that might be suitable to your skillset.\nIf you are feeling particularly adventerous, put that info (that you are looking for a new role) in your Twitter display name. People use that all the time to advertise all sort of things\nOf course, don’t become a spammer. Only reach out personally to people whom you’ve built a connection with AND that plausibly can help you. As long as you are tactful and don’t hope for a response, you should be good.\nRemember – don’t forget to tell the world what you want!\n\n\n\nMagical Approach : “Value up front”\n\nDeep truth – Everyone in life and on the Internet thinking about themselves. So what is the absolute best way to stand out from the self-interested crowd? - Do something completely selflessly for someone else!\nBut If you are doing something seemingly nice and useful with the intetion to promote YOURSELF, or you expect gratitute, or to show off, you will be ignored (as you should).\nOn the other hand, if you go about your life and think – “oh, this person is doing something interesting, is there something I could do to help?”\nThen, This is such an unusual event that it unlocks limitless opportunities!\nHow do you help other people ?\n\nYou look at what that person is doing and see if you can add genuine value.\nOh, they wrote a blog post? Maybe I can share my appreciation via retweeting it?\nOh, they seem to be maintating a software library? Maybe I can jump in and see if I can resolve an issue or two.\nDo not introduce yourself.\nDo not ask for the time of people you don’t know or where you are adding ZERO value to their undertaking.s\nYou just go around and be a good citizen AND DON’T EXPECT ANYTHING IN RETURN.\n\n\n\n\nHow to Think about - “ What To Work On ? “\n\nLet’s face it – the only way to ensure we do something over a long period of time is to make it fun.\nif to become successful you need to do stuff you don’t want to (AKA force yourself to do stuff AKA the whole discipline thing) what is the point of changing your life in the first place?\nIsn’t the whole idea behind achieving success that you have more control over how you spend your time, that you can do more activities that are “fun” for you?\nLife is a journey not a destination and if you can’t make the journey fun, you are losing on the only game worth playing.\nEnjoyment is the key to Excellence.\n\n\n\nSmall Bets\n\nBeing aware of how much effort something will require is super important. For instance, let’s look at Kaggle.\nIt is by far the best place to learn ML, to become one of the best in the world at ML.\nBut incremental gains, going from being in the top 5% to top 0.1% requires a lot of work. Plus, what is very important – even if you are a Kaggle Grandmaster with 20 gold medals to your name, even if you understand how a problem should be approached, you still have to invest weeks of work to find yourself at the top of the LB.\nAnd that time can be very valuable.\nat every step of the way I plan to be aware of how much time and other resources whatever I end up doing will require and what results I’ll be getting!\nThis is likely to not only help with how efficient I am with investing my resources, but should also help with how intentional I am about living my life!\n\n\n\nBeing an Eternal Student\n\nIf you are not making money through your side projects, then why go to all this effort at all?\nYou can use the stuff that you create to build your personal brand. Nothing will showcase your skills better than a project you ship and write about.\nBut the true reason to work on side projects goes deeper than that. And that is that they are the best training grounds for acquiring new skills and honing the ones you already have!\nThe learnings are the true currency of side projects and maximizing them is what I want to focus on!\n\n\n\nStrategic Mediocrity\n\nThere are so many people around doing wonderful things.\nHow do they do it? Are they fundamentally smarter/better than me? Are their actions THAT good to warrant attaining results I would consider a success?\nWhile that would be a very optimisitc view of the world (that people are good at what they do) nothing could be further from the truth!\nA perspective where you have to be good, you have to be great, to achieve success is paralyzing and a total MYTH\nThe true world doesn’t work like that. You don’t get better by trying to be better. You get better through experiences. Through iteration. Through action.\nAnd the only way to get a lot of action under your belt is via accepting mediocricity.\n\n\nthe main failure mode is not not being good enough, but not performing at all in the first place.\n\n\n\nThe True Alignment Problem\n\nHow can I tell whether my side projects are bringing me the benefits I am after?\nHow can you evaluate your own side projects?\nOne way to approach this would be to list out the things you care about and see whether you are moving closer to them or not.\nBut the review process needs to be ongoing. While working on something, or between projects, you have to be constantly monitoring whether you are on the right path or not and be ready to change your approach as needed.\nIn fact, this point is broader than just ensuring you are on the right track.\nAs you work on things you have not worked on before you will learn new things about the world. Maybe you will find that something is harder or easier to achieve than you originally assumed.\nOr maybe you will learn that doing things in public carries less risk than we generally think it does. For instance, in our connected world we often worry whether what we say here or there might impact our professional prospects.\nBut the truth is that most people don’t care at all, they barely notice what we are doing.\nAs we go along and collect all these learnings it is fundamental that we update our interal map of the world. And this, just as ensuring we are on the right track, can only happen through introspection.\nThrough a continuous and deliberate process of reflecting on our experience (be that via discussing it with others, or writing about it).\n\n\n\nOne last theme to bring it all together\n\nProduce indiscriminately, consume selectively\nThere is so much noise on social media. Everyone is so loud and the messages are so flashy. The best this, the legendary that.\nPlus it all is oh so interesting!\nAnd yes, there are many extremely valuable thoughts floating around. But it is next to impossible to get at them while sifting through piles of content where each piece is meant to trigger our emotions.\nThat is why you have to be selective when it comes to the information you consume.\nBooks, Courses, Research papers, Tutorials, Documentation, Blogs, Forum discussions, Writing down and sharing my thoughts, ML Experts podcasts are much better investment of time as compared to watching AI news , entertainment etc.\nIf you are mostly a consumer you are missing out on creating a lot of value for yourself.\n\n\n\nTips on becoming a better programmer (as an MLE)\n\nI now don’t see it as learning syntax, learning about programming paradigms or design patterns. For me, becoming better as a programmer now always starts with what I already know, even if it is very little, and learning on top of it via iteration / trying things out. Maybe - —\nwriting something somewhere about what I learn to structure my thoughts (though in a class room I guess one can also talk to people which must be nice “. One thing I don’t do enough for sure is reading other people’s code - this is probably the best way to learn. - —\nRefactoring! I don’t think I ever heard this discussed in an academic setting, but this is one of the most important practices / skills. This book is great https://refactoring.com - it explains the entire philosophy (probably no value in going too deep on the techniques when someone is just starting out, but the understanding that this is 99% of what writing maintainable code is about is invaluable."
  },
  {
    "objectID": "private/mle-guide/index.html#zach-mueller",
    "href": "private/mle-guide/index.html#zach-mueller",
    "title": "The Cracked MLE Guide",
    "section": "Zach Mueller",
    "text": "Zach Mueller\n\nHow to become good MLE like Zach\n\nIf you were starting out today what would you focus on and you wished you knew earlier in this DL journey ?\n\nGet. Good. Software. Engineering. Skills.\nSame with math, strong code skills helps everything much easier in this space.\nDo the Fastai courses. Follow Jeremy’s advice precisely.\nRead all of Stas’ Book end to end, and watch all the DL from Foundations (fastai 2019 course)\n\nWhat are the most important things you learned from mistakes?\n\nHow to debug well, through brute force learning of it.\n\nFavorite Learning resources :\n\nPython Distilled\nFastai Book + DL from Foundations (fastai course 2019)\nStas Bekman Book\nClean Code (Uncle Bob)\nA philosophy of software design (John Ousterhout)\nThe Art of Readable Code"
  },
  {
    "objectID": "private/mle-guide/index.html#stas-bekman",
    "href": "private/mle-guide/index.html#stas-bekman",
    "title": "The Cracked MLE Guide",
    "section": "Stas Bekman",
    "text": "Stas Bekman\n\nMachine Learning: LLM/VLM Training and Engineering by by Stas Bekman\nThe Art of debugging\nThe AI engineering battlefield (guide)"
  },
  {
    "objectID": "private/mle-guide/index.html#sairam",
    "href": "private/mle-guide/index.html#sairam",
    "title": "The Cracked MLE Guide",
    "section": "Sairam",
    "text": "Sairam\n\nHow would I learn machine learning today\nOne of the most frequent questions I get is “How would you learn machine learning if you were starting today?”. I thought that it might be worth it to actually write it out and share it with all of you. Here’re 5 things I’d do if I were starting over today…\n\nTip #1: Get Going\nIf you’re unsure where to start and feel intimidated by the math, take a code-first course. If you feel excited by theory, take a theory first course. The most important thing is that you start.\n\n\nTip #2: Build your Fundamentals\nDon’t worry about which framework to learn. Pick whichever one you like and run with it. What’s really important is that you learn the fundamentals. Pytorch, Tensorflow, and Jax are all frameworks that help you solve problems, but the fundamentals that they implement are the same. Focus on those first. Every time.\n\nFrameworks come and go, fundamentals come and grow.\n\n\n\nTip #3: Read and Implement Papers\nForget trying to keep up with all the advances in the field. Machine learning moves way too fast. Instead, understand the implementations of papers you’re reading and reimplement them yourself. That will help you tremendously and compound your learning.\n\nThe best papers of today will become the textbooks of tomorrow.\n\n\n\nTip #4: Learn Publicly\nWrite about what you learn and share it publicly. You don’t need to be an expert. There are a million others who have the same questions you do. Make their lives easier. Share your knowledge. This is the step that everyone skips. Learning in public will build connections that you previously may have missed.\n\n\nTip #5: Be a T-shaped learner\nDive Deep into your area of interest like Computer Vision but explore other areas like Natural Language Processing, Recommenders, and Graph learning.\n\nGreat research ideas are the byproduct of cross pollination.\n\nIf you’re not sure where to start, Begin your Kaggle journey seriously. Competing with the best in the world will raise your level and will accelerate your learning. ## 5 Steps to 10x your Machine Learning Productivity\n\n\n\n5 Steps to 10x your Machine Learning Productivity\n\nStep 1 : Rule of Three\nAsk 10 people how they’d handle multiple projects & deadlines and 9 of them will tell you to multitask.- This is not the solution.\nInstead, each day, write down the top 3 things you want to get done. These could be things that are time-sensitive or maybe are difficult problems you need to solve. Our brains aren’t great for storing information. They are great at thinking and problem solving.\nMulti-tasking forces the brain to juggle and remember tasks and where they are in the execution cycle.\nWriting 3 things down unburdens your brain and allows it to focus on what it’s great at — problem solving. Additionally, you get the nice dopamine hit when you finish and check off an item from your list. Any item that isn’t fully completed at the end of the day gets rolled over to the next day’s list.\nIf you do this, here are the results you can expect to happen:\nDeep Work: Uninterrupted spells of flow state allowing you to do your best work on a task.\nLess context switching: No more switching between tasks when you’re in the zone. Additionally, no more background threads about the other task(s) you left unfinished while you’re working on the one at hand.\nNo procrasti-planning: Procrasti-planning is the constant mental gymnastics your brain does to organize the order in which things need to be done. Eventually, nothing gets done because you spent all your time and energy on planning and not on doing. Simply put, it’s procrastination masquerading as planning.\n\n\nStep 2: Bitter Pilling\nOnce you’ve got your 3 things, here’s the next problem you’ll likely face. We all have tasks on our plates that are boring, monotonous or painful to finish. For me, these are data annotation, cleaning, writing automation scripts and reports. Your instinct might be to finish off the tasks you enjoy first and then come to these painful blisters last.\nDon’t put the most boring thing off for last.\nGet it done first. Swallow the bitter pill. Why? If you do it first, you can enjoy the rest of the tasks in peace. Furthermore, you’ll be happy to get this work done soon so that you can devote your time to intellectually stimulating work. Another pitfall you’ll avoid is that you won’t repeatedly postpone doing them until your plate is filled ONLY with these kinds of tasks.\n\n\nStep 3: Batching a.k.a Pipelining\nAfter you’ve accomplished Steps 1 and 2, now it’s time to pipeline. If you work with deep learning models, you’ll be familiar with the amount of time it takes to train and fine-tune them. What I’ve found is that these long windows of waiting are ripe for pipelining.\nWhile you have experiments that you are waiting on, do something else in parallel.\nFor example, I read papers, answer emails or go get my workout done while my model trains.\nModels aren’t the only ones that work well with batches. You do too.\n\n\nStep 4: Owl or Lark?\nAt this point, you’re now ready to tackle peaks & troughs. Each of us have unique windows in a day when we are at our maximum effectiveness (peaks) and minimum effectiveness (troughs). The trick is to schedule our work so that we prioritize high impact work in these optimal windows.\nHere’s what to do:\n• Do your high impact work like coding, brainstorming, researching when you are at your peak effectiveness\n• Do low impact work like automation, answering emails, logging bug fixes, etc. outside of peak hours\nOwls are those who are most effective at night. They thrive when their high impact work is done in the night. Larks are those who are most effective during the day. The converse is true for them.\nAre you an Owl or a Lark? Find out and maximize your effectiveness. Do your best work at your peak hours\n\n\nStep 5: Build a bridge\nThat’s it! Now it’s time to build a bridge for the next day. A simple mistake I used to make was to just drop what I was doing at the end of a day and come back the next day to resume. A side-effect of this was that it took me a good half hour to identify what I was doing the previous day and what I had to do next.\n\nFind a good stopping point for unfinished work: Make it easy to pick up the next day\nWrite a note to yourself on what you need to look at next. That way, you don’t start the next day with a blank slate\nCelebrate your accomplishments before you close\n\nOverall, The reason this framework works so well is because it’s repeatable and your work compounds with less friction and mental burden. So, don’t fall into the trap of being a multi-tasking juggler.\nDo this instead.\n\n\n\nPart-Time Learning for a Full-Time Career:\nWherever you are in your career — A student, a fresh graduate just into work, or a grizzled veteran, there is one mindset that you should never abandon. That is the growth mindset. A key component of the growth mindset is the habit of constantly learning new things. This is incredibly relevant for machine learning practitioners.\n\nSo, how do I constantly learn you ask?\nLearning anything new is hard. While our brains enjoy the pursuit of knowledge, we experience inertia during the early stages. We then use the algorithm in our head that I call the “Logical Explanation of Procrastination” to justify our unwillingness to learn. You may be familiar with some of the reasons this algorithm provides:\n\nI have limited time since I have ______ (The blank is usually filled with too much work, family commitments, etc.).\nI don’t know where to start.\nLearning is an isolated pursuit and I don’t have the motivation.\nI easily get distracted and don’t have the discipline\nInsert your reason here\n\nAdditionally, most people think the solution is to either adopt the “hustle” mindset and burn the midnight oil (and the candle at both ends by extension) or quit their job and learn full-time.\nThese proposals aren’t sustainable. I learned machine learning (and deep learning eventually) while working full-time. I made a ton of mistakes and hit many walls along the way. Eventually, I came up with a sustainable approach to learning and I figured that might be useful to you.\n\n\nJust-In-Time learning\nLearn something new exactly when you need it and bring together the resources you need as you are learning. This might seem a bit confusing. You could argue that you don’t do machine learning at work, that the topic you want to learn isn’t related to the research you do, or that you’re looking to learn machine learning to switch careers. What then?\nThis technique still works. How?\nJust-In-Time learning is project-based. The project doesn’t need to be what you are doing at work (If it is, then that’s awesome). It just needs to be something you want to do.\nSimply define a project you want to pursue, say an image stylizer or a speech synthesis system, and work back from the goal. Find papers that solve this problem and read them. Augment your understanding through repositories in GitHub that solve the problem or a sub-problem. Work back from the goal until you have every single step mapped out.\nThe key is to only collect and use resources that are directly relevant to your end goal. Ignore everything else.\nThis ensures that you don’t waste time searching and getting buried under a never-ending pile of resources. So, we have a starting point. How do we deal with the lack of time?\nFor this, I take a leaf out of James Clear’s book Atomic habits. There are 4 laws of habit formation:\n\nMake it obvious\nMake it attractive\nMake it easy\nMake it satisfying\n\nApplying each to Just-In-Time learning, we get:\n\n\nMake it obvious:\nYou only collect resources that help with the end goal, so there’s nothing unclear as to what you should ignore. Every step you need in this learning journey is obvious.\n\n\nMake it attractive:\nIf the goal of your project is work-related, then the rewards are explicitly laid out. If on the other hand, you are looking to transition into a career in machine learning, the #1 thing that companies look for is proof of work. There’s NO bigger proof of work than a body of GitHub projects, open-source contributions, etc. Courses and transcripts only go so far. At the end of the day, companies want to know you can actually do the work hands-on.\n\n\nMake it easy:\nWork on it, piece by piece, regularly. Even if you only have 30 minutes a day to devote to this, that’s fine. Show up. Your learning compounds over time. By committing to a manageable chunk of time every day, you make it easy for yourself and remove the “lack of time” excuse from your quiver of reasons to not learn.\nDo the math: 30 days x 30 minutes per day = 900 minutes of focused learning\n\n\nMake it satisfying:\nThere’s an intrinsic satisfaction you get by just showing up. When you master a hard skill, you gain confidence, and each new thing you learn helps you learn harder things more easily. Learning is its own reward.\n\n\n\nOne of the Best Ways to Improve Your ML Skills :\nLet me tell you a secret:\n\nThe single most effective habit that helped accelerate my learning wasn’t just reading papers, books, or watching lectures.\n\nIt was actively studying others’ code.\nI ran code reviews of other repositories. This allowed me to learn how various algorithms were implemented and pick up best practices to use in my own work. This is how I did it (and so can you!):\n\nStep 1: Implement the Algorithm Yourself\nBefore jumping in to see how someone else has implemented a model or an algorithm, do it yourself first. It doesn’t need to be perfect. Just write out how you think the algorithm works and test it.\nAverage case: It works. Best Case: It fails.\nWhy is that the best case? You will be more open to learning from another repository. If you’re anything like I was, the moment something worked, I’d immediately close my mind to exploring how it could be done better. It wasn’t until much later that I changed my perspective on how writing good code is an endless process.\n\n\nStep 2: Find Good (and Bad) Examples of the Algorithm\nThere are a million repositories on GitHub that implement the same thing. How do you find out which ones to learn from? There are 3 simple options I choose from:\nOption 1: Use the Official Implementation\nIn machine learning, often times the algorithm you implement comes from a paper. That naturally means that there is a high chance that the code for the paper was released along with it.\nStart there.\nYou can learn how the authors implemented various parts of the algorithm straight from the source.\nOption 2: Look up the Algorithm on Papers With Code\nIf you can’t find an “official” implementation, the next best bet (and in fact, this should be option 1) is to look at Papers With Code.\nThis site has a huge collection of papers with various repositories that implement them. In addition to the paper, you can see the top-ranked repositories that implement the paper.\nOption 3: Use the Number of Stars as a Guide\nIf the authors didn’t release the code (boo!), and you can’t find it on Papers with Code, then the next best bet is to search with the phrase “INSERT NAME OF ALGORITHM HERE GitHub”. Then, use the number of stars the repository has as a guide to choose a starting point. Usually, a repository with a higher number of stars ( &gt; 1000) is a safe option to learn from.\nThis isn’t a hard and fast rule though.\nThere are many hidden gems out there that just haven’t gotten the eyeballs on them yet.\n\n\nStep 3: Learn from the Contrast\nYou now have the repository (or repositories) you’d like to learn from. The next step is to learn the differences in implementation. Crucially, just focus on the core implementation first.\nFor example, if you are trying to learn a new model architecture, focus on just the part of the code that implements the architecture.\nWith the paper at your side, see how each part has been implemented by this repository and contrast it to how you’ve implemented it. This isn’t to say that what you have done is wrong. Rather, observe the contrast in styles, usage of idiomatic code, brevity, and clarity.\nNext, take notes on things you don’t understand on the first attempt. Why did they implement it this way? What is this new syntax they’ve used? How are they accounting for X? These are all great questions to think through.\nOne of my favorite repositories that implement the vision transformer is by Phil Wang. In the image below, look at the highlighted line. When I first came across it, I had no idea what that syntax was. Turns out, it’s something called Einstein Summation, or einsum for short. It’s a super efficient way to manipulate tensors.​\n\nNow, I use this when relevant in my implementations. This is just one of the many things you can do when studying a new repository.\nNot All Repositories are Created Equal\nRepeat after me. “It’s ok to study poor code”. 🧐\nWhile this might surprise you, look at what a legendary author says about the same thing in another domain:\n\n“Every book you pick up has its own lesson or lessons, and quite often the bad books have more to teach than the good ones.” ~ Stephen King\n\nYou can learn a lot from poorly written code. It tells you exactly what not to do, why not to do it, and how not to do it. Bad code helps you clearly identify the pitfalls to avoid and how you can save hours if not days by writing good code in the first place.\n\n\nStep 4: Expand Your Toolbox\nYou’ve now done all the hard work of studying and analyzing a set of good and bad implementations. But your job isn’t done. The last step of the process is perhaps the most important.\nReflect on your notes and your learning. Augment your toolbox. In particular:\n\nWhat are the techniques I can use going forward?\nHow can I make my implementation more readable and clear?\nWhat are the things I should avoid when implementing this?\n\nThe next time you implement something, try to incorporate some of these learnings and see how they work in action.\n\n\nTL;DR\n\nImplement it yourself\nFind Good (and Bad) Implementations to study\nLearn from the Contrast\nExpand Your Toolbox\n\n\n\n\nMachine Learning and You - in the tech Industry\n“When patterns are broken, new worlds emerge.”\nThere’s never been a better time to transition careers into the ML space. Seriously. The best time was 10 years ago.\nThe second best time is now!\nThere are a few common misconceptions I’d like to tackle before getting to the various roles that might interest you as a practitioner.\n\nMisconception #1: You need to have a PhD\nThe short answer is that you don’t. The long answer is you don’t for 99% of the roles. A Ph.D. in machine learning is needed IF and only IF you want to break into the top tech research labs as a research scientist. For just about any other role, you don’t need one (unless you really want one and would like to be addressed as Dr. INSERT_YOUR_NAME_HERE).\nThe primary reason a Ph.D. is valuable in my opinion is that going through a rigorous Ph.D. program teaches you how to do systematic research, and how to formulate abstract ideas into concrete problems that can be solved.\nI don’t have a Ph.D., and I’ve transitioned from a systems engineer to a research engineer to a research scientist (humble brag, sorry). The reason I bring it up is that you need to know that this isn’t a barrier to entry. You can and you should apply to roles that interest you, AND for which you have the skills to deliver on the job.\nWhat you need is a curiosity itch, the ability to code, and the willingness to learn new things. These three skills will help you break into big tech, and grow in the machine learning space.\nSo, repeat after me - “I don’t need a Ph.D. unless I absolutely want to go out and get one.”\n\n\nMisconception #2: You need to be really good at math\nMachine learning is math-heavy. I agree. But, you don’t need to know all the math in the world to get started. In fact for some roles in ML, you’ll be so far removed from any kind of math that you’ll wonder why you didn’t transition sooner. I won’t lie, there’s some high school math that’s really important. You can learn that on the job as and when you need it.\nDon’t let this stop you either. Please.\n\n\nMisconception #3: Your past career experience is useless\nOne of the most beneficial skills to acquire in any kind of machine learning role is domain expertise. So whether you’ve been an accountant, or have worked in pharmaceuticals or any other field that seems remote to ML, know this - Your domain skills will actually help you, not hinder you.\nHere’s why.\nThe hardest skill to acquire is business intuition. Not ML. If you can’t communicate why you need to use machine learning, or how it would solve your business problem, you won’t get buy-in from all the stakeholders.\nYour domain expertise will let you speak to non ML folks in a language they understand!\nSo treat your past experience as a superpower, not as wasted effort.\nWith these out of the way (read them aloud if you aren’t convinced yet), let’s look at the two broad categories of machine learning roles:\n\n\nMachine Learning Research:\nThe goal of research is to push the boundaries of the field further and to solve fundamental problems. Essentially, the team may go, “Alright folks, let’s see if this claim is valid or not and go down this rabbit hole”. Some research may lead to products, BUT, that isn’t why research is pursued.\nCompanies also have a flavor of research called applied research. This is usually directed to solving practical problems the company has, and may have shorter time frames.\nResearch roles are of two kinds:\n\nResearch Scientist: This is probably the only role in ML that usually requires a Ph.D. There are circumstances where a high performing engineer with a good track record of top-tier publications gets into this role, but those are far and few. A Research Scientist comes up with novel ideas, puts them to the test and publishes them. Additionally, they act as advisors to other engineers in the team.\nResearch Engineer: This role isn’t present in all companies. Sometimes a company might use Research Engineer and Research Scientist interchangeably. The Research Engineer usually works in collaboration with the Scientists and runs experiments that evaluate ideas they come up with. This role doesn’t require a Ph.D. or top-tier publications.\n\n\n\nMachine Learning in Production:\nAs the name suggests, the goal of production is to come up with a product - be it a good or service. Machine learning in production usually deals with a need that the company’s users have. Naturally, the constraints are different. In research, you might focus on beating the state of the art. In production, however, your goals may be ensuring that the ML system runs fast, reliably, and works well with the other parts of the product.\nThus there are more hats to don, and more opportunities to enter the field.\nBelow are a few different production roles I’ve seen in the industry:\n\nMachine Learning Engineer (MLE): The MLE is a software engineer who has ML skills or can learn ML skills needed for the role. Their goal is to design, build and productionize ML models for business needs. In this day and age, this is one of the coolest roles to have. Your work directly impacts the product.\nData Scientist: A data scientist needs to form their own questions about the data and use machine learning or predictive modeling to answer them. Data science is a deep and rich field. As a data scientist, you’ll turn data into business insights and help the organization make better decisions. The key phrase is unlocking meaning from data.\nData Engineer: They make it easier for others in the organization like Data Scientists, and MLEs to use data for their needs. To do this, a data engineer builds systems to collect, manage, organize and analyze data at scale. Raw data is hard for anyone to use, and in this age of big data, managing and creating an effective structure for the rest of the organization to use is an invaluable skill.\n\nThere are some newer roles emerging like ML ASIC Engineers, Infrastructure engineers and so on which might also interest you depending on your background.\nOverall, there’re a ton of roles that you can choose from depending on your skills and interests. You just need to start.\nKey Takeaways:\n\nYou need just three things to get into the ML field:\n\nA willingness to learn\nThe ability to code\nCuriosity\n\nThere are many opportunities for you - Find the right starting point\nDon’t wait to learn everything and then start. Learn what you need. Learn the rest on the job."
  },
  {
    "objectID": "private/mle-guide/index.html#rohan-rao",
    "href": "private/mle-guide/index.html#rohan-rao",
    "title": "The Cracked MLE Guide",
    "section": "Rohan Rao",
    "text": "Rohan Rao\n\nHow do you manage time?\n\nFocus on Prioritization & Sacrifice and have the Discipline to Plan & Pursue. I believe it is a very underestimated skill.\nI have described the process in detail in my CTDS interview:\n\nHow to choose the right ML model and parameters?\n\nSet up the right validation framework - [[Good Validation Set]]\nExperiment on as many models, architectures, hyperparameters\nPick the one that best suits your needs (accuracy, stability or latency are usually the choices)\n\nIs statistics a major pre-requisite for learning ML?\n\nDesire to learn new things. Perseverance to read, code and experiment hands-on. Willingness to invest time and energy.\nI’d pick these as more impactful pre-requisites over statistics.\n\nWhat helped you improve concepts in DS?\n\nIn order of impact:\n\nDesire to learn\nInvestment of time\nGoogle searches / YouTube videos\nKaggle forums / notebooks\nSource codes / GitHub repos\nResearch papers\nFriends\n\n\nResearch vs Implementation priority in ML?\n\nYou need to nurture and grow your knowledge in both.\nWithout research, how do you know what to implement?\nWithout implementation, how do you know the impact of research?\nThey are two sides of the same coin.\n\nWhich is your favourite ML model?\n\nIn this fast-moving ML field I don’t believe in marrying any algorithm or technology.\nMy favourite model is the one that gives me the best results!\nHaving static favourites can stunt growth. Let it change based on the data."
  },
  {
    "objectID": "private/mle-guide/index.html#trybackprop-mle-faang",
    "href": "private/mle-guide/index.html#trybackprop-mle-faang",
    "title": "The Cracked MLE Guide",
    "section": "TryBackprop (MLE @FAANG)",
    "text": "TryBackprop (MLE @FAANG)\n\nHows the market saturation of AI/ML engineers these days and how to overcome it ?\n\nAt FAANG, there isn’t too much saturation. In fact, for the most part, the AI/ML engineer role was resilient to the tech layoffs and the hiring freezes of the past two years (of course, there are exceptions if you’re underperforming). What I found interesting was even during “hiring freezes”, FAANG made an exception for senior level ML engineers.\nThe market is likely saturated with underskilled AI/ML engineers, so to overcome it, after you get some ML experience, to set yourself apart from the others, get familiar with the fundamentals. It’s not enough to know how to use the ML libraries like PyTorch/TensorFlow. You must also understand why they work so that you encounter production issues with real world systems, you can debug them more easily. It’s not a good use of time nor is it feasible to read through all real world systems code so you need to sharpen your ML reasoning skills. To help, Andrej Karpathy’s zero to hero series on YouTube is incredibly valuable: https://youtu.be/VMj-3S1tku0?si=SI7f8BnJDmQleBkf. For context, Karpathy graduated from Stanford with a ML PhD, and he worked at OpenAI and Tesla as an AI director on their self driving car effort.\nAs for engineers with zero experience in ML, it helps to land a role in an ML team where you’re working on regular software engineering. At least you’re close to the ML action. I’ve seen many engineers take that approach. Then, at home, continue to learn ML at home when you have time. Eventually, once you’ve built up enough context at work, you can ask your manager for a role change within the team or at least start working directly on ML projects. I know this works because my coworkers did this, and I’ve done this as well.\n\n\n\nHow is job market for freshers? I heard somewhere that you need to have previous work experience in data science or any other domain to enter into ML. Is it hard to directly get hired as ML fresher without any work experience?\n\nThat’s not true, though it does help to land a FAANG role. If you have little to no experience, you can continue to work on side projects and possibly land contract roles or roles at smaller companies that are willing to take a chance on you (since they can’t afford to hire FAANG qualified MLEs yet). As you build experience with the projects and the startups, you can hop to FAANG in a year or two if you’ve learned the fundamentals well and have gotten good experience.\n\n\n\nAny project suggestions? How to land a job?\n\nProject suggestions – code up a convolutional neural network and train it on the MNIST dataset. Most people can’t even do the basics like that. Then code up a transformer decoder and train it on the tiny shakespeare dataset (you can watch Andrej Karpathy’s excellent video on this: Let’s build GPT: from scratch, in code, spelled out. - Karpathy | YouTube). If you accomplish these two, you’ll for sure generate plenty of great project ideas to put on your resume along the way. Most people will lose motivation to accomplish these two coding projects.\nHow to land a job – go to ML panel events. Talk to the panelists. Ahead of time, do some research on the panelists so that when you chat with them, you can impress them and feed into their ego that you know what they’re working on. If you have also done some side projects, be sure to mention it. They’ll likely ask for a resume and see if you’re a good fit. Landing a job requires both knowledge of the ML and being a hustler and networking with folks. Most of people I know who landed an ML role after working in traditional software engineering simply networked and knew the right people. You can network from anywhere in the world as long as you have an internet connection.\nIf you want a role in AI/ML, my suggestion would be to figure out what type of company you’d want to join (in terms of size). If you want to join FAANG, you can certainly apply, but if they reject your application, don’t be discouraged at all. I would personally apply to smaller companies and startups, build up some experience there, and if the startup does well, you’re benefit. If the startup doesn’t do well, at least you’ll have had some experience working in ML, and you can apply to FAANG, which will value that experience.\n\n\n\nWhat level of math do you use in your daily work? What’s the most frequently used ML concept, library or tool in your work? Imagine If you’re a student graduating this year, how will you prepare for an ML job interview?\n\nlevel of math – Day to day, I use at most algebra level math. If you can solve a system of 7 or 3 equations, you’re good. Probably about three, four times a year I’ll need to deeply reason about a system and calculate derivatives of single or multi variable equations, but even then, it’s pretty simple.\nMost of the time I rely on my stats and probability knowledge, which you can learn if you were to take the first 4 weeks of any U.S. undergraduate course on stats/probability.\nML concept – Know your neural network fundamentals well.\nLibrary – PyTorch, Python\ntool – online monitoring tools to monitor traffic and neural network diagnosis tools\ninterview – I’d follow this former OpenAI scientist’s curriculum that he established at OpenAI for almost all folks new to ML: https://github.com/jacobhilton/deep_learning_curriculum\n\n\n\nSince AI field is developing so fast how do u stay up-to-date?\n\nThe AI field is developing fast, but the major breakthrough concepts come out every few years, so you can spend most of your time on the breakthrough concepts and not feel like you’re drowning in every new paper that’s coming out.\nFor example, you should spend way more time on the “Attention Is All You Need” paper by Google that introduced the Transformer than you should on the latest paper that just came out yesterday. Plus, once you study the major breakthroughs and know them well, you start to notice that the other ideas are just derivatives of the breakthroughs and require just one or two tweaks of the breakthrough idea.\nFor example, I spent about 3 months trying to understand all the nuances of transformers. Then, I spent about 2 weeks building one from scratch and training it on a tiny dataset and getting it working. After that, reading the papers on GPT-1, GPT-2, and GPT-3 were relatively easy (less than 1 hour each). At that point, learning about Llama 1, 2, and 3 became a very quick scan of the paper and noticing what changes they made to the transformer and noting which changes were worth diving deeper into. This knowledge builds on itself and compounds so once you study the breakthrough ideas, the rest come relatively easy. Furthermore, you build up more confidence in yourself that you’re absorbing new concepts faster and faster.\nAlso, I talked to my friend who’s a researcher at Deepmind and my other friend who’s a researcher at OpenAI, and they both independently told me that most of the papers that come out are bogus, and you just need to talk to the experts to know which ones to pay attention to. If you don’t have access to the experts, simply look at a paper’s number of citations, and if it’s in the thousands, it’s a good signal that it’s an important paper.\nYou’ll also know a paper’s worth if there are plenty of implementations of it on Github. Of course, some papers have no open implementation, which doesn’t mean it’s a worthless paper. One time one of my coworkers showed me an efficient and fast way to implement e^x so that our Android code would run faster and wouldn’t use as much power (and thus save battery power for the user). I looked up the paper that originated the fast implementation and it had very few citations, yet it was a very useful and powerful technique!\n\n\n\nWould familiarizing oneself with Data Engineering concepts be helpful?\n\nYes I’ve found data engineering skills to be very valuable so early on i decided to pick up those skills so that i would not be blocked on data engineering work for my projects. The skills made me immensely more efficient at work.\nRecommended Resources :\n\nData Engineering zoomcamp (great course)\nDesigning Data intensive applications Book\nDesigning Machine Learning Systems - Chipro Book\n\n\n\n\nAre jobs more geared towards mlops now, are ML engineer/jobs still focused on dsa/ml/ design ?\n\nDefinitely! The ML engineers I work with focus on DSA, ML, and system design. In fact, that’s most of what we do. I also I have teammates that run the ML ops part of the system, but day to day, my coworkers and I are designing new models, implementing papers/faster algorithms to reduce compute and memory usage (which is very constrained these days due to the chip shortages) and designing new monitoring and measurement frameworks that require knowledge of probability, stats, and basic linear algebra.\n\n\n\nIs maths helpful ? ML maths looks daunting to learn ?\n\nTo answer your question, yes the math foundation definitely pays off later on. However, if you lose motivation to continue because you feel it’s a slog, you can skip straight to the ML coding and whenever you get stuck on a math concepts, simply learn it then. That way you won’t feel like you’re learning math with no sight of the end of the tunnel.\n\n\n\nWhat do you see at FAANG for folks who want to pursue say DL compilers, GPU programming, distributed training etc etc? (Just picked a few but I hope you get the gist).\n\nI know a couple engineers who hopped around from general ML to GPU programming, distributed training etc. They took it upon themselves to learn CUDA programming, organize tech talks with experts from Nvidia, and proactively took on challenging GPU programming/distributed training problems that very few in the company could solve. They also found opportunities in other teams and switched when they wanted to take on different responsibilities. As a result, they were rewarded handsomely by the company since their passion in these areas resulted in massive savings for the company as well as more cutting edge research and product development.\nFor GPU programming, FAANG experts recommend reading Paulius Micikevicius’s Nvidia blog: https://developer.nvidia.com/blog/author/paulius-micikevicius/. Google “Paulius Micikevicius GTC” if you want to learn more. Furthermore, I recommend listening to the PyTorch developer podcast: https://pytorch-dev-podcast.simplecast.com/episodes/all-about-nvidia-gpus.\nIf you want to dive deeper into ML systems engineering, these resources are very helpful: Chip War – NYTimes best selling book by Chris Miller Asianometry – YouTube channel with 667k subscribers by Jon Y SemiAnalysis – tech journal with 95k+ subscribers by Dylan Patel\nAuthor: Paulius Micikevicius | NVIDIA Technical Blog\nhttps://pytorch-dev-podcast.simplecast.com/episodes/all-about-nvidia-gpus\n\n\n\nAny tips for becoming a “ML Infrastructure engineer”?\n\nBecoming an ML Infra engineer is relatively simpler jump actually. I would highly recommend reading this book if you want an accurate picture of what its like being an ML infra engineer. “Designing ML Systems Production Ready”!.\n\n\n\nHow to learn the most relevant DL fundamentals ? is Leetcode imp ?\n\nYes, I think Andrej’s videos give you a good grasp of modern ML technology. You can then add on from there.\nI would prioritize leetcode when you’re 2-3 months away from interviewing to give yourself ample amounts of time to study 1 or 2 problems a day or so. Shorten the amount of study time if you don’t mind cramming.\n\n\n\nGrind Leetcode or ML ?\nDo as little Leetcode as you can get away with, But Read as many ML papers as you realistically can.\n\nDo as little leetcode as possible (1.5-2 hrs everyday)\n\nOnce you are able to solve medium level LC problems, &gt;&gt; Pay less attention to LC\n\nRead ML Papers consistently\n\nThe goal here is - Consistency over quantity\nAt start ; Commit to - Reading 1 ML paper per week.\nGradually make it 2 papers a week &gt;&gt; 100 papers per year\n\n\n\nHow to Prepare for ML interviews ?\n\nFree ML Resources to Become a FAANG ML Engineer\nDSA: prepare with https://neetcode.io/practice. Do the neetcode 150\nML : go through this curriculum by a former OpenAI researcher , https://github.com/jacobhilton/deep_learning_curriculum\nFAANG Interview – System Design\nFAANG Interview – Machine Learning System Design"
  },
  {
    "objectID": "private/mle-guide/index.html#breaking-into-ml-as-a-new-grad",
    "href": "private/mle-guide/index.html#breaking-into-ml-as-a-new-grad",
    "title": "The Cracked MLE Guide",
    "section": "Breaking into ML as a New Grad",
    "text": "Breaking into ML as a New Grad\n\nThe Misconception\nOne of the reasons so many people struggle with getting a job in AI/ML is the hype and the “seemingly” low barrier of entry. It is trivial to complete the Andrew NG course on Coursera and run a few Colab Notebooks. Also, libraries like Hugging-Face make it easy to run models like Stable Diffusion without the need to understand the years of research, mathematics, and engineering that went behind it. This is good for people who want to try out these models, researchers doing multiple quick experiments, and even artists. Running scripts and completing online courses takes little to no effort while giving a false sense of being interview-ready.\nReal effort is required when you start to understand the mathematics behind the models, the engineering skills required to serve these models, the data engineering skills required to collect and clean the data for these models, and the research to create new models.\n\n\nRequired Skills\nI often hear people express their desire to become “ML engineers,” when asked what that entails, they typically respond with “wanting to train ML models.” However, for many, this simply means watching the loss decrease during training. In reality, training an ML model is a complex process that requires a team effort, with engineers possessing one or more specialized skills.\n\nResearcher (usually a PhD) with papers in conferences like NeurIPS, ICML, CVPR, EMNLP, etc.\nExperience optimizing and serving models in production.\nStrong background in data engineering, such as ETL, data warehousing, data cleaning, etc.\nStrong background in infrastructure engineering, such as managing large clusters, load balancing, autoscaling, etc.\nExperience in training models, including deciding the architecture, hyperparameters, loss functions, etc.\nStrong background in hardware engineering, such as designing ASICs, FPGAs, etc.\n\nLooking closely, most of these skills are not exclusive to ML; they are transferable across various software roles. As a result, many ML roles require at least 3-5 years of experience in a software-related position. But then how does a fresh grad break into this field? A classic “chicken or egg” situation.\n\n\nFresher’s Delimma\nAlthough AI/ML jobs usually require the above general skills, a few are exclusive to AI/ML and are required when working in domains like LLM and Diffusion. Companies looking to expand or build their AI/ML teams usually seek people with the following skills.\n\nHave a deep understanding of the mathematics behind models, such as backpropagation, optimization algorithms, loss functions, activation functions, etc. Don’t just read them; implement them in Python or CUDA.\nCan solve the Blind 75: Contrary to common belief, Leetcode is equally important to crack ML roles. I was asked leetcode style questions by almost every company. Practicing NeetCode 150 (Easy and Medium) helped me answer most interview questions.\nHaving a good internship experience: Internships are usually easier to get than a full-time job and add much value to your resume.\n\nThese skills are crucial, but given the competitive nature of the field, it’s essential to have something that sets you apart.\n\nFirst author publications as a Bachelor/Masters student in top ML conferences (publication in a no-name conference won’t help). Since a publication is not easy, good research experience is also highly valuable.\nOpen-source contributions: Like research, open-source contributions are highly valued in ML. For freshers, starting with open-source can be intimidating due to the large and complex codebases. One strategy is to start with issues labeled “good first issue”. They are easier to start with, and you can proceed from there as you become familiar with the project. However, contributing doesn’t always mean diving into these massive projects. Another approach is to create your repository; it can be minimal implementation of FlashAttention, a “Chat with PDF” app, LLM in pure CUDA, or SoTA quantization algorithms. People learning these topics can benefit from your work, and your repositories may gain attention and stars.\nNiche skills required in the ML industry, such as Quantization, CUDA programming, Compilers for ML, etc. These skills take time to learn but are highly valuable.\n\n\n\nInterview Questions\nBelow is a list of questions I was asked in the interviews for the above-mentioned skills. They are not company-specific.\n\nLeetcode-style Algorithms: Backtracking, Detecting cycle in a directed graph, Finding subarray with the maximum sum, 2 Sum, Implementing BFS, DFS, Weighted traversal, Create a dictionary from a list of words (Trie), Topological sort, Find if an event can be scheduled with given prior events, use python decorators.\nML coding in Python: \n\nDecoder-only transformer model (asked by around 7-8 interviewers)\nBatch Normalization\n2-D convolution in Numpy with padding and stride\nBack-propagation in 2-layer NN\nSimple RAG pipeline.\n\nTraditional ML: \n\nWhat is the difference between L1 and L2 regularization?\nDifference between RNN, LSTMs, and Transformers?\nHow do you handle class imbalance?\nHow do you handle missing data?\nHow to handle overfitting/underfitting?\n\nDeep Learning: \n\nExplain Layer Norm, Batch Norm, RMS Norm, etc., and when should you use one over the other?\nHow do you handle vanishing/exploding gradients?\nExplain the Attention mechanism and the intuition behind it.\nExplain activations like ReLU, Swish, GLU, GELU, etc.\n\n(ML) System Design: \n\nHow do you serve a model in production (Approach is as a traditional sys design problem)?\nExplain the KV cache and the memory requirements for using it?\nWhy is Flash attention needed, and how does it work?\nHow do you deal with terabytes of data for training?\nDesign an active training pipeline ?\nWhat is Continuous and dynamic batching?\nHow do you process millions of documents?\n\nDiffusion: \n\nHow does a Diffusion model work? (Can explain using score-based modeling, Flow-based modeling, etc)\nHow is it different from GANs, VAEs, etc? How is it different from autoregressive models like LSTMs, Transformers, etc?\nExplain sampling techniques like DDPM, DDIM, etc. What is classifier-free guidance? Have you ever trained a Diffusion model?\n\nLarge Language Models: \n\nExplain Attention and the intuition behind it.\nExplain various types of position embeddings like RoPe, Absolute, etc.\nWhy is a product of the Q and K matrix normalized?\nHow is LLaMA different from GPT-2?\nWhat is LoRA?\nHow do you finetune a model?\nWhat are the memory requirements to serve an LLM?\nHow do you evaluate a LLM?\nWhat is RLHF, and why is it needed?\n\nQuantization: \n\nExplain Quantization\nExplain or implement AbsMax\nZero-point, AWQ, Smooth Quant, their drawbacks\nQuant training techniques like QAT, PTQ, etc. What is Perplexity?\n\n\nYou’re not expected to know all of these topics. The questions you’ll face will vary based on your job description, experience, and the projects listed on your resume.\n\n\nReading Paper 📄\nSome interviewers gave me a research paper on the spot and told me to read and explain it in 20 minutes. Since it is not possible to read the entire paper in such a short time, I first read the abstract, followed by the methodology, results, and finally, the conclusion. Then, if there was some time left, I read the introduction and the related work.\n\n\nGeneral Suggestions 📍\n\nAvoid using tools like GitHub Copilot during interview preparation; they make you lazy. Writing a code from scratch takes effort; Copilot is okay to use while working full-time.\nUse LinkedIn wisely: Most interviews I got were through cold messaging a hiring manager who posted a job on LinkedIn.\nWhile referrals can be helpful, don’t wait too long for someone to submit one on your behalf. Applications might close before you receive the referral. Another strategy is to apply as soon as a job is posted. This increases the chances of your application being seen by the recruiter/manager.\nKeep learning: The AI/ML job market and the required skills are dynamic. Keep yourself updated with the latest advancements and research. I have found that following high-signal accounts on Twitter (this list) is a good way to stay current.\nPlay to your strengths: Don’t chase after the “ML” job title, especially in today’s market. If you excel in a domain outside of ML and want to transition, looking for opportunities while continuing in your current area of expertise might be wise.\nDon’t overpromise your resume: It is easy to mention a model used in a project without a complete understanding. The interviewer usually expects you to know the model in depth, including how it works, the training methodology, the dataset used, results, etc.\nLuck plays a big role. Even if you perform well in every interview, you might still face rejection. Companies often don’t provide feedback after a rejection, so it’s important to take some time to reflect on what might have gone wrong, learn from it for your next interview, and then move on.\nFinally, keep your family close and your friends even closer.\n\n\n\nConcluding thoughts\nThis post is not meant to discourage anyone. AI/ML is still a nascent field with much work to do. Work Hard. Don’t just sit and watch YouTube tutorials or run Colab notebooks. Do tasks that require active effort, like leetcode, good research, implementing new algorithms from scratch, and making positive contributions to the Open-Source community (OS contribution takes effort; don’t do it just for the sake of it). Have faith in yourself, and you’ll get there."
  },
  {
    "objectID": "private/mle-guide/index.html#mark-saroufim",
    "href": "private/mle-guide/index.html#mark-saroufim",
    "title": "The Cracked MLE Guide",
    "section": "Mark Saroufim",
    "text": "Mark Saroufim\n\nHow to get a great job\n\nThe most common questions I get asked :\n\n“I didn’t study in the US, how can I get a job in the US?”\n“Do you think I need a PhD? Or Masters?”\n“I just need to start working at BigLab to work on X”\n“I get too stressed in tech interviews, what should I do?”\n“Isn’t it too late for me to switch careers?”\n\nThankfully, all these questions can be approached in the same way.\n\n\nHow writing saved my sanity\n\nEarlier in my career at Microsoft, I wanted my day to day tasks to align perfectly with my interests at the time. This is a challenging proposition as projects take a long time to deliver whereas my interests are a lot more fickle often influenced by whatever happens to be favored by the Hacker News ranking algorithm.\nWhy can’t I find someone to talk about causality libraries to, isn’t there anyone building a category theory deep learning library, what about differentiable differential equations?\nThankfully I’ve learnt that it is both unfair to expect my colleagues to share all my interests but it’s also counterproductive. Instead, it’s best to reach out and meet people online that are already doing the kind of work I find interesting.\nThe key to getting people to respond is asking interesting questions and the best way to ask interesting questions is to have a good high level overview of a field and the best way to do that is to write a blogpost or make a video about the topic.\nWriting has a ridiculous number of hidden advantages from clarifying your thinking to making your name more recognizable online but it’s by far the best way to meet like minded people online.\nIf you want career advice, you can write a good question to people you admire on Twitter on LinkedIn.\n\n\n\nYour goal should be to apply to jobs with a portfolio not a CV.\n\nHowever, a portfolio is much harder to beef up than a CV because you need to actually produce lots of visible work.\nBut before you actually produce unique and valuable work you need to learn how to at least copy the work of others. So share what you learn and turn it into an asset, whether it’s a Tweet, YouTube video, Twitch livestream — keeping a public journal of everything interesting you’ve been learning will keep you motivated and give you an easy way to actually start producing unique and valuable work.\nThe point is that you can produce an impressive portfolio regardless of whether you choose to attend college or not.\nSo instead of spending months trying to crack coding interviews or spamming recruiters — - Start building a portfolio of things you’re learning, modify them and eventually make them better. You’ll learn more, meet like minded peers, get hiring managers reaching out to you and most importantly you’ll be far happier.\nOptimizing for an amazing CV has a tremendous amount of luck involved, optimizing for a portfolio is entirely within your control — you just need to spend a few minutes everyday.\nDon’t stress if the outcome doesn’t match your expectations, maybe you won’t develop a widely popular open source library but here’s a secret neither have most hiring managers.\nConsider that there’s only a few hundred popular open source projects that exist at any given time and each of those libraries is generally maintained by 1–2 people (including Pytorch) so the reality is most people can’t actually produce robust open source software even though lots of people think it. It doesn’t mean you shouldn’t try. You’ll still learn a lot by trying and if you succeed you may not need a job.\nDoes this approach work for everyone? Probably not. But it worked for me so I can’t help but recommend it. However, I don’t buy the time that portfolios bias towards rich candidates with lots of free time\ntech interviews take loads of time to prepare and also bias towards rich candidates with lots of free time. The difference is portfolios are actually indicative of ability.\n\n\n\n\nHow I Read Technical Books\n\nDon’t keep reading a boring book — if you take 1 thing from this entire post let it be this. The idea of “completing” a book only applies to novels or textbooks for a class. You never really finish understanding a subject and if it’s also boring you’re unlikely to be good at it. Boredom and procrastination are information not a quest that needs to be conquered so try to keep yourself honest about whether you actually want to read a certain book. Related: I don’t read 500+ books because it’s more likely I’ll find multiple 200 pages ones that are far more pleasant to read.\nTake lots of notes in the book — ink is a lot less valuable than ideas so I stopped treating books like collectibles and treat them as consumables. The more beat up a book looks, the more love it’s gotten by its owner.\nAsk smart people for book recommendations — the best books on a topic are rarely just a book on the topic instead they give you a new way of seeing the world. So if someone I admire recommends a certain book, I buy it .\nRead and highlight your interests in the table of contents — the table of contents generally makes it clear what is background material (usually in the appendix). I make a note next to each chapter that looks fascinating to “me” and just read those.\nDO NOT READ BOOKS COVER TO COVER,— My favorite technical books are the ones I come back to the most often. I first do a quick scan of the chapters I’m interested in and make a list of open questions or concepts I don’t understand.\n. Then read the chapter again and try to answer those questions and make a new list of questions.\nSkip the middle pages of a chapter on a first read — the beginning of a chapter often motivates and defines the problem and the end summarizes it. That’s usually all I need if I’m treating a techniques as a black box. I do this first, it takes a few minutes only and then read the middle part if I’m really compelled to do so.\nSkip the middle pages of a chapter on a first read — the beginning of a chapter often motivates and defines the problem and the end summarizes it. That’s usually all I need if I’m treating a techniques as a black box. I do this first, it takes a few minutes only and then read the middle part if I’m really compelled to do so.\nThe first 3 chapters often contain 80% of the total information — It’s very rare for long books to be building on a single idea. What’s more common is that the first few chapters introduce the basic terminology and the basic formulation of the problems you’re working on and then the rest of the chapters branch out from this base. Exception to above rule: it’s OK to read the intro chapters cover to cover.\nBuild a glossary — each field has its own basic terminology and basic results. Just knowing what those words are may not make you an expert in the subject but it’ll make it possible for you to go through more complicated work if it’s interesting to you. It’s very common for some words not be well defined in some books in which case just find alternate references on Google and YouTube and write down inside the book cover what those words mean.\nPrerequisites are often overestimated — many books will say something like “prerequisites are linear algebra and calculus” but what they really mean is you should look 3 theorems online before you start. Tackling prerequisites partially is totally valid.\nWrite notes — this can be as simple as a bullet list of things you thought were cool or it could be a long blog article. I usually reread great chapters about 3 times so it takes me about 3 iterations to get to a blog post I’m comfortable sharing.\nAvoid the Hivemind — if you’re an independent researcher odds are you’re on a tight budget so you need to be smart about how you allocate your time, don’t try to be the best in the world at 1 specific topic and compete with the top research labs in the world all by yourself. You’ll get diminishing returns after investing too much in a single field so you’ll probably be a lot happier browsing techniques from various fringe fields instead of the 1 over-hyped one.\nGithub Archeology — whether a book is talking about simulating some model or describing some algorithm it’s almost impossible to understand how they really work by visual inspection alone. Google “ Github” and you’ll often find top notch implementations of any technical problem you can think of. The best code-bases are interactive books so a lot of the advice from above also applies.\nIt gets easier — different fields will often use the same mathematical techniques, ideas and code. Because ideas cross pollinate you’ll find it easier over time to read technical books regardless of the subject matter. Before you start seeing some connections between fields you’ll find yourself questioning the point of what you’re reading so it’s important to have some faith that some connections do exist because more often than not, they do.\n\n\n\nMachine Learning : The Great Stagnation\n\nMatrix Multiplication is all you need\nI often get asked by young students new to Machine Learning, what math do I need to know for Deep Learning and my answer is Matrix Multiplication and Derivatives of square functions. All these neuron analogies do more harm than good in explaining how Machine Learning actually works.\nLSTMs a bunch of matrix multiplications, Transformers a whole bunch of matrix multiplications, CNNs use convolutions which are a generalization of matrix multiplication.\nDeep Neural Networks are a composition of matrix multiplications with the occasional non-linearity in between.\n\n\nEmpiricism and Feedback Loops\nI’ve learnt “the hard way” that Deep Learning is an empirical field, so why or how something works is often anecdotal as opposed to theoretical.\nThe best people in empirical fields are typically those who have accumulated the biggest set of experiences and there’s essentially two ways to do this.\n\nSpend lots of time doing it\nGet really good at running many concurrent experiments\n\nAge is a proxy for experience but an efficient experimentation methodology allows you to compress the amount of time it would take to gain more experiences.\nIf you have a data center at your disposal this further multiplies your ability to learn. If you and all your peers have access to data centers this is yet another multiplicative feedback loop since you can all learn from each other.\nThis helps explain why the most impactful research in Machine Learning gets published in only a few labs such as Google Brain, DeepMind and Open AI. There are feedback loops everywhere.\n\n\nFast.ai\n\nPutting the user first is the approach that Fast.ai took when building their Deep Learning library. I think of Jeremy Howard as the Don Norman of Machine Learning.\nInstead of just focusing on the model building part, Fast.ai builds tools around all of the below.\nBy tools I don’t mean a black box service, I mean software design patterns specific to Machine Learning. Instead of Abstract Factories, abstractions like Pipelines to chain preprocessing steps, callbacks for early stopping all the way to generic yet simple implementations of Transformers. Design patterns are more useful than black boxes because you can understand how they work, modify them and improve for both yourself and others.\nHonorable mention to nbdev which aims at removing some of the common annoyances of working with notebooks by eliminating all obstacles that would get in your way of shipping your code as a library including a human readable git representation, continuous integration and automated PyPi package submission."
  },
  {
    "objectID": "private/mle-guide/index.html#george-hotz",
    "href": "private/mle-guide/index.html#george-hotz",
    "title": "The Cracked MLE Guide",
    "section": "George Hotz",
    "text": "George Hotz\n\nHow to become an ML engineer ?\nWell, here’s the steps to get good at that: 1. Download a paper 2. Implement it 3. Keep doing this until you have skills\n\n\nHow to learn Programming\n\nThe only way to learn programming is…\nI have something I wanted to do , and I tried to do it.\nI am like…Okay..It would be nice…If the computer could do this thing..\nThats how you learn - You just keep pushing on a project."
  },
  {
    "objectID": "private/mle-guide/index.html#aleksa-gordic",
    "href": "private/mle-guide/index.html#aleksa-gordic",
    "title": "The Cracked MLE Guide",
    "section": "Aleksa Gordic",
    "text": "Aleksa Gordic\n\nTo become good ML Engineer :\n\nIf you truly want to become proficient with machine learning (I really don’t like the word expert) try to get out from the “going through the newest courses and books” phase as soon as possible.\nToo many people keep on reading the newest books that come out…\n(and same for courses), thinking they are now up to date with ML world whereas in reality they are “light years” behind (things move fast around here .\nTry to get into the paper reading and replication phase as fast as you can without skipping the necessary steps 👇\nBuild up some fundamentals first by going through Fastai course\nfocus on building your own projects…\n(they don’t have to be paper replications per se - be creative!)\nBut even at this point, if you are only reading papers from the conferences you’re still lagging behind (~6 months) the latest and greatest that is popping up on arxiv as I’m writing this (which is again a lot of time in the ML reference frame).\nObviously this is mostly an advice about the modeling part which moves the quickest. If you want to learn about e.g. MLOps then there are some books you have to read (like@chipro’s) and the rest is…doing it in your daily work.\nNo arxiv papers for that. :) Also - learn some software engineering! Extremely underrated. Our projects are becoming increasingly engineering-heavy (scale is all you need paradigm). Take LLMs or any recent text-to-image synthesis models as examples.\n\n\n\nInput & Output leaning Cycles\n\nInput Cycle\n\nFor 2-3 months , I learn by taking courses, finish its excercises , assignments, build notes, anki cards.\n\nOutput Cycle\n\nNext, For a week-month (depending on Task) , I learn by -\nWriting Blogs\n\nSummarising everything I learnt in DL , NLP , any topic etc.\n\nAbout my Kaggle solutions, experience\nAbout understanding & Implementing Papers\nAbout Personal Projects\nAbout Implementing ML/DL algorithms\n\nKaggle 🦆\nBuilding projects\nContributing to Open Source Project\nImplement fundamental/seminal Research papers.\n\nThis keeps in check, that i am implementing my knowledge - by solving problems, producing knowledge"
  },
  {
    "objectID": "private/mle-guide/index.html#greg-brokman",
    "href": "private/mle-guide/index.html#greg-brokman",
    "title": "The Cracked MLE Guide",
    "section": "Greg Brokman",
    "text": "Greg Brokman\n\nHow I became a machine learning practitioner\nIt’s time to become an ML engineer"
  },
  {
    "objectID": "private/mle-guide/index.html#vicki-boykis",
    "href": "private/mle-guide/index.html#vicki-boykis",
    "title": "The Cracked MLE Guide",
    "section": "Vicki Boykis",
    "text": "Vicki Boykis\n\nHow I learn ML\n\nHow I learn machine learning | ★❤✰ Vicki Boykis ★❤✰\nAll you have to know as an MLE is :\n\ncomputer science fundamentals: data structures, algorithms, object-oriented programming\nan in-depth understanding of production engineering best practices: monitoring, alerting, unit testing, version control, latency, throughput, scale\nfundamentals of machine learning and statistics: traditional models, sampling, A/B testing\nan informed opinion about all the currently popular trends in a machine learning stack so you can select from them\nan understanding of the developments in the field you specialize in (medical AI, recommendations, search, ML for security, each of which are formulated as different problems and have their own context and vocabulary)\nYAML\n\nLearn the basics\n\nThere are lots of concepts to learn in ML engineering. But the concepts are not distinctly unique.\nWhen I started thinking of learning as a process that involves going extremely wide on core fundamentals, then going deep and building links across fundamentals, it made it the process a LOT more manageable.\nExample 1 : once you learn OOP in Python, you’re going to have much easier time understanding Fastai library.\nExample 2 : graph data structure\n\nA graph, like all data structures, has special properties: it’s not linear, it has nodes, the numbers, and edges, which connect the numbers.\nFirst we go broad: In order to understand a graph, we have to understand what the alternatives to it could be: It’s different from other data structure, like arrays, lists, and hashmaps. So now we’re covering algorithms.\n once we understand graphs as a data structure, we understand that trees are a form of graph.\nAnd once we understand trees, we understand that DAGs, one of the canonical data structures in data data work, are a type of tree, as well, and can also be traversed.\nThis gets us now from abstract data structures to things that we work with every single day: Airflow DAGs, execution plans in Spark, and Tensorflow/PyTorch computational graphs.\nAlso , we can grasp Graph neural Networks , a little fast.\n\nExample 3 : Numpy Arrays\n\nonce you understand Numpy arrays, you understand how they build to Pandas DataFrames and PyTorch tensors.\n\nSo just by learning one foundational concept, we unlock being able to work with it across various domains within machine learning and relating concepts to each other. We can also now reason about what data structure might work best where and what kinds of engineering tradeoffs we make when implementing X or Y.\nThe more foundational concepts you learn, the more you put into your working memory to synthesize with what you already have available and the easier it becomes to create connections\n\nAs a quick primer, the human brain has several types of memory, short-term, working, and long-term. Short-term memory gathers information temporarily and processes it quickly, like RAM. Long-term memory are things you’ve learned previously and tucked away, like database storage. Working memory takes the information from short-term memory and long-term memory and combines them to synthesize, or process the information and come up with a solution.\n\n\neverything you read in machine learning and engineering repeats.\n\nSo, if you don’t understand the first time, don’t feel bad. Just pick up a different book (or three). You will already have the foundations of the first book to guide you."
  },
  {
    "objectID": "private/mle-guide/index.html#chip-huyen",
    "href": "private/mle-guide/index.html#chip-huyen",
    "title": "The Cracked MLE Guide",
    "section": "Chip Huyen",
    "text": "Chip Huyen\n\nMachine Learning Interviews Book\nMLOps guide"
  },
  {
    "objectID": "private/mle-guide/index.html#umar-jamil",
    "href": "private/mle-guide/index.html#umar-jamil",
    "title": "The Cracked MLE Guide",
    "section": "Umar Jamil",
    "text": "Umar Jamil\n\nHow Umar Jamil learnt MLE\nRead as many papers as possible (since AI is evolving so fast, no book will teach you everything u need to know) - Always Be Coding (ABC) :\n- Solve Problems:\n\n\n\n\nShare your work\n\nWrite Blogs\nBut , there are lots of blogs/videos already on topics. What to do ?\n\nI force myself to never watch anything that is already produced on the topic I want to produce about I have never watched the video about flesh attention before making my video about flesh attention interesting I have never watched a video about Transformers before reading my videos on Transformers that’s interesting right because if you want to produce something unique you don’t want to be influenced by others otherwise they set the threshold for you.\n\n\n\n\nResearch without a PHD\n\nShape Suffixes — Good Coding Style | by Noam Shazeer | Medium\nNoam Shazeer didn’t even finished his bachelors degree.\n\n(co)Inventor of (Transformer), MoE, Multihead Attention, Multiquery Attention, Tensor-Parallel LLM Training, SwiGLU, etc. Previously @Google,\nHe built @Character.AI\n\nSarah Pan - works at answer.ai (as a researcher) just after highschool.\n\nShe learnt Deep learning with Fastai course part 1 + 2\n\nAlec Radford during his undergrad, created DCGAN, core-contributer to initial Pytorch, built GPT1-2.\n**Not having a PHD degree is not issue.\nJust build a strong portfolio with projects, opensource-contribs, blogs etc.\n\n\n\nLessons learned as a Research Scientist\n\nFollow a T-shaped learning (as Andrew Ng told in his Career in ML lecture)\n\nHave some fundamental knowledge in CV, NLP, DL, classical ML, GPU programming..etc\nAnd Build Expertise in one of them.\n\n\n\n\n\nHow Andrej taught himself ML engineering\n\nDon’t start coding without an end goal.\nCoding Llama from scratch - is great to learn LLM fundamentals.\nBut nobody will ever pay you to code llama from scratch you know what they will pay you for is take llama and try to quantize it in such a way that it retains as much as per the performance as possible\nSo always build things whatever it is at an API level at the machine learning level with an end goal\n\nfor example: imagine you are creating a company for a machine learning model inference who do you need the guy who on hugging phase just on a weekend delivered a quantized version of a model that you want to put on your service or the guy who can code llama and never quantize the model and never influenced the model ever in his life so building means do something with an end goal\nthink always in this terms of business like I am going to launch a business that is doing machine learning inference or trying to solve a problem What problems will I have I will have probably the problem of okay how can we Host this model and make it as fast as possible or there is a new model and this context length is only 8K can we take it to 16k so let me do it.\nI know they follow tutorials and they make projects yes but you are just copying some code that is online did you actually deliver something because if I am the founder of a startup I can do that myself I need people who solve problems for me\nSo learn to solve problems\n\nTake any model and you start beating a benchmark\nYou take a model and you say okay this model is not really performing on a medical data so let me take a data set on medical data and try to improve the performance of this model on medical data trying to retain as much knowledge as possible\nThe Learning Journey that will take you to beat The Benchmark will be enough for you to enter this field\n\n\nthat’s what karpathy did. He wanted to learn about training of language models so he started writing a little script in Python to train a language model then he was pissed off because he said I don’t really understand what is happening under the hood then he started coding it in C and it called the he called his repository llm.c\nthen he realized hey I can train this stuff I can understand this stuff and people start contributing to it and then he realized hey there are these things called cuda kernels I can write these Cuda kernels and they make it go fast then he realized hey I can go very deep into kernels so just by teaching himself something which is I want to train a model and this is the problem I want to solve and I want to do it the fastest way possible he teach him self all the field actually he teach himself all the things that he needed to master this field.\n\n\n\nHardest challenges\n\nWhen you nobody and want to enter AI field - you get inferiority complex. How to tackle it ?\n\nBy Believing in yourself – By putting yourself in challenges & coming out of it.\nexample :\nTake challenges\n\nFinish Fastai part 1,2 courses & Fastbook.\n100 days CUDA challenge\ntake up Kaggle competition - and atleast come up with end to end solution.\nfinish the Andrej’s NN zero to hero course.\n\n\nHow to read MML (mathematics for machine learning) book\n\nTry to read the applied chapter + the maths concepts needed (together)\nThe Bottom Up (goal based approach)\n\nHow to extract as much knowledge from the little resources ?\n\nThis is a fundamental research skill.\neg: Implementing Transformer paper without watching any tutorial (just by reading paper and code)\nHow to do it ?\n\nWhenever you want to learn something - ground your learning path around A GOAL.\n\n\n\n\n\nUmar’s journey : learning CUDA, triton & Flash Attention\n\nWhat is CUDA, Triton, Flash Attention ?\n\nFlash Attention is an optimised implementation of attention mechanism used in Transformers to make better use of GPU compute power. In the paper it’s refer to an “I/O-Aware” algorithm.\nCUDA is a software stack from NVIDIA to write GPU kernels for NVIDIA GPUs. Other GPU manufacturers have their own software stack (the equivalent for AMD GPUs is ROCm)\nTriton is a software stack that compiles GPU kernels written in Python into native code for the underlying GPU hardware (CUDA, ROCm, both compatible as of now.)\n\nI want to learn Triton : How do I start ?\n\nBefore you start learning anything , ask yourself : “Why do I want to study triton ?”\n\nIn Umar’s case : he like experimenting with new architectures and with new architectures you may need new kernels for accelerating the training process. Too many times I was blocked because i didn’t know how to edit/debug the [[Kernel]]s.\n\nNow its time to go to &gt; Triton’s website\nOh! I am confused what’s this ?\n\nYou can never get lost: What’s this ?\n\nAnytime you are blocked somewhere and you don’t understand something - you can always prompt\nAnd you can never be blocked - because you can keep prompting untill you’re unlocked ! They will put you in right direction.\nPrompt &gt;&gt; [[What is Triton and How can i learn it ?]]\n\nGet your hands dirty !!\n\nYou learn more by doing things and not just studying it (conceptually).\nLook at Triton Tutorials.\n\nSet yourself a goal/dream\n\nUmar saw Fused Attention coded in 400 lines of python code - and made a goal to understand it + re-implement it himself (in his Triton learning journey).\n\nPrompt &gt;&gt; [[How is Flash attention and fused attention related ?]]\n\nAlways “Have a goal when you learn things”\nDon’t just learn Triton.\nLearn Triton in order to understand the Fused Attention.\n\nWhy having an objective is important when learning ?\n\nYou always have something to measure against.\nYou never get lost.\nIt gives you sense of satisfaction when you reach your goal.\nIntermediate Goals also helps alot.\n\nLearning Strategy: Avoid being influenced by others’ tutorials initially. Challenge yourself. Define the “rules” of your learning challenge\n\nWhat are the rules of this objective ?\nAm I allowed to read papers, watch tutorials, blogs, code ?\nThe difficulty of the challenge you choose - you’ll improve confidence+competence accordingly..\n\nPractical Steps: Started with the “vector addition” tutorial on the Triton website.\n\nNow we are trying to do the vector addition tutorial and we found an obstacle (Encountered concepts like pointers, block size, and thread IDs.) and we found resources about on how to kind of overcome this obstacle\nOvercoming Obstacles: When blocked, use the debugger or prompt LLMs to identify missing knowledge.\nTargeted Learning:\n\nYou are doing triton , to understand flash attention\nYou are doing vector addition (implemented in Triton’s first tutorial, on triton website)\n\nLearn just what you need , no more, no less\nWhen seeking external resources (like the “Programming Massively Parallel Processors” book), learn only what’s relevant to the immediate task (just learn enough for us to understand the things that are relevant to the vector addition only)\nI went to pmpp I I learned the first two chapters I think enough for you to understand vector addition and to give you a little intuition on all the concept that you need to at least understand fully.\n\nFinal Boss: Finish the tutorials, then move on to the “final boss” - flash attention.\n\nWhen we start reading (Flash Attention) paper, we encounter so many new terms… So let’s go down rabbit holes ?\nUntill now - our strategy was - every time we encounter an obstacle - we ask chatgpt for directions ; overcome the obstacle and go back to objective.\nPaper Reading strategy is different.\n\nLearn to handle the uncertainity.\n\nRead the entire paper , top to bottom , even if you understand 5% of it.\nWhile reading the paper , you’re constantly reminded of how many things you don’t understand and this makes you uncomfortable: its fine. Keep Going On!\nOnce you reach the end , you’ll have a clear vision of what you don’t know, and now you can come up with a plan for how to fill your knowledge gaps.\nTip: Highlight with different colors &gt;&gt; [[Color coding research papers]]\n\nthings you don’t understand at all and you’d like to learn\nthings you don’t understand and you’re fine not understanding (because maybe are not so relevant for your journey)\nthings that you already know & you’d like to improve your knowledge on, etc.\n\n plus your brain is actually very smart at connecting dots even on things that you are not not fully familiar with.\nDon’t highlight while reading.. Highlight after reading.\n\n because this forces you to go through it again of course not every paper you can do it like this so it depends really on the level of depth that you want to reach\n\nRead the paper atleast 3 times – end to end.\n\nNow you know your objective - to understand all the necessary concepts/parts of this paper.\nNow you can go into rabbit hole of learning each of those concepts & you always comeback to the paper.\n\n\n\n\nLearning Roadmap: Built a roadmap by identifying knowledge gaps in the Flash Attention paper (softmax, online softmax, GPU programming, backpropagation).\nWhat ingredients do we need to understand FA ?\n\nAttention mechanism\n\nSoftmax\nSafe Softmax\nOnline Softmax\n\nGPU programming\n\nHBM and Shared memory\nTiling\nTensor shapes\nCUDA/Triton\n\nBackpropagation\n\nGradients\n\n\nFirst Boss : Softmax\n\nIf you go to theflash attention paper, the key challenge in implementing Flash Attention, particularly related to computing softmax when processing attention matrices in blocks.\nThe core issue with block-wise computation of attention is handling the softmax normalization factor correctly.\nThe problem arises because softmax requires knowledge of all values in a row to compute the normalization denominator (the sum of exponentials). When processing in blocks to save memory, you don’t have all values available at once.\n![[Pasted image 20250310193729.png]]\n\nThe two weapons to fight softmax :\n\nSelf-Attention Does NOT need O(n^2 ) memory\nOnline normalizer calculation for softmax\n\nReading is not enough. You need Active Learning.\n\nIf there is an algorithm , code it.\nif there is an algorithm , test it on paper.\nif there is a proof :\n\nre-write it yourself on paper , the first time.\nif you want deeper understanding, try to redo the proof without having it infront of you.\n\n\nbut I still didn’t understand it… What should I do ?\n\nImagine you studied the online softmax, coded the algorithm, did the proof by yourself, but you still can’t figure out how it may be connected to FA .. What should you do ? Should you keep studying “online softmax” or should you continue ?\nAfter a reasonable time spent on a problem , Continue Forward and accept the uncertainity & the possibilty that you may have to go back to it again in future.\nIterative Learning: Go back and forth between different resources (e.g., the Flash Attention paper and the online softmax paper) to reinforce understanding.\n\nwhat I do usually in these cases is go back and forth so I go to the flash attention paper try to understand how it’s used then if I don’t fully understand go back to the online softmax because every time you go back and forth\nso when you go back to the online softmax from the flashh attention paper you are reinforcing your learning of the flashh attention by trying to make it work with whatever you’re learning from the online softmax.\n\nCombining Code and Theory: Alternate between reading the paper and examining the code for new architectures.\n\nWhen I read a new architecture paper ,i don’t fully understand it just by reading the paper. Hence i go back and forth between the code and the paper.\nSometimes The code is very cryptic as its written by researchers , not by good engineers, hence learn from paper and code together.\n\n\nBlock Matrix Multiplication: Learned about block matrix multiplication, crucial for understanding Flash Attention’s block-by-block computation.\nTensor Shapes and Strides: Understanding tensor shapes and strides is essential for GPU programming, as GPUs work with memory pointers.\nBackpropagation: The most challenging part, requiring understanding of gradients and chain rule.\n\n\n\nWhat should I learn next ?\n\nWhat should I study next in order to grow in my career ?\nFirst rule : Follow curiousity , not hype.\nSecond rule : Focus on consistency. it doesn’t really matter how bad your start is, with enough consistency, with enough trial and error, you can fix any intial bias in your learning.\nThird rule : if your learning speed hasn’t accelerated by using LLMs , youre doing something wrong.\n\n\n\nHow to keep up with research pace ?\n\nEveryday we see 100s of papers published. How to cut the noise ?\nAccelerated Learning:* Modern tools (LLMs) significantly accelerate learning. Prompting replaces time-consuming searches.\nDual Stream Learning:* Keep up with new research (“noise stream”) but maintain a long-term learning focus.\nSecret Ingredient, Confidence:* The most crucial element is you. Build confidence by tackling challenging tasks. Push yourself beyond tutorials.\nBuilding Confidence:* Confidence comes from doing hard things, and pushing yourself further.\nLeaderboard Participation:* Encourages participating in the GPU Mode leaderboard, even if not fully confident. The only failure is not showing up.\n10,000 Hours Rule:* Is the only rule that matters in life."
  },
  {
    "objectID": "private/quotes/quotes.html",
    "href": "private/quotes/quotes.html",
    "title": "Favorite Quotes",
    "section": "",
    "text": "You start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die."
  },
  {
    "objectID": "private/quotes/quotes.html#random-favs",
    "href": "private/quotes/quotes.html#random-favs",
    "title": "Favorite Quotes",
    "section": "",
    "text": "You start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die."
  },
  {
    "objectID": "private/quotes/quotes.html#ayn-rand",
    "href": "private/quotes/quotes.html#ayn-rand",
    "title": "Favorite Quotes",
    "section": "Ayn Rand",
    "text": "Ayn Rand\n\n“[Dean] “My dear fellow, who will let you?”\n[Roark] “That’s not the point. The point is, who will stop me?”"
  },
  {
    "objectID": "private/quotes/quotes.html#depth-first-procrastination",
    "href": "private/quotes/quotes.html#depth-first-procrastination",
    "title": "Favorite Quotes",
    "section": "",
    "text": "You start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die."
  },
  {
    "objectID": "private/quotes/quotes.html#found-on-internet",
    "href": "private/quotes/quotes.html#found-on-internet",
    "title": "Favorite Quotes",
    "section": "",
    "text": "You start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die."
  },
  {
    "objectID": "private/quotes/quotes.html#thoughts-from-internet",
    "href": "private/quotes/quotes.html#thoughts-from-internet",
    "title": "Favorite Quotes",
    "section": "",
    "text": "You start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die."
  },
  {
    "objectID": "tils/quotes/quotes.html",
    "href": "tils/quotes/quotes.html",
    "title": "Favorite Quotes",
    "section": "",
    "text": "You start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die."
  },
  {
    "objectID": "tils/quotes/quotes.html#thoughts-from-internet",
    "href": "tils/quotes/quotes.html#thoughts-from-internet",
    "title": "Favorite Quotes",
    "section": "",
    "text": "You start with a high-level goal , and rather than taking the optimal route, a combination of intellectual curiosity and anxiety about standing on shaky foundations leads you to spawn subgoal after subgoal after subgoal, until you hit rock bottom and you find yourself doing some task so distantly related to the original goal you’re in another universe entirely.\nAnd it’s always bullshit. Labouring in obscurity on some grandiose scheme for years is overrated. Tighten the feedback loop: trim the fat, ship, react, iterate. Do or die."
  },
  {
    "objectID": "tils/quotes/quotes.html#ayn-rand",
    "href": "tils/quotes/quotes.html#ayn-rand",
    "title": "Favorite Quotes",
    "section": "Ayn Rand",
    "text": "Ayn Rand\n\n“[Dean] “My dear fellow, who will let you?”\n[Roark] “That’s not the point. The point is, who will stop me?”"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I am Saurav.\nI’m a Computer scientist based in India. I love Deep learning, math, and wisdom literature.\nI’m currently on my arc to become an ML engineer .\nI believe in learn in public. As a result , I am documenting and sharing all my learnings here."
  },
  {
    "objectID": "private/mle-guide/index.html#how-to-learn",
    "href": "private/mle-guide/index.html#how-to-learn",
    "title": "The Cracked MLE Guide",
    "section": "How to learn",
    "text": "How to learn\n\nThe most valuable kind of knowledge is the type you get from experience. So how do you get a ton of it?\nAnswer : Maximize doing new unfamiliar things. You’ll get lots of new, useful, experience-based knowledge. The kind you get from years of experience. The kind you cant find on Google.\nExperience comes from experiencing things, not from time. If you passively wait around, you’ll barely get any. The opposite is true, too – if you actively seek out experience, you’ll get way more of it.\nExperience-based info\n\nExperience-based info is the extremely valuable type of info you get from doing new, different things.\n![[Pasted image 20250703091152.png]]\n\nSome forms of experience-based info you might learn from new things:\n\nuseful details that fill in a map of the territory your business operates in, showing you better directions to move in (this is why pivots happen)\nknowledge that a specific fear you had is not actually anything to be afraid of\na skill (knowledge of how to do something) that permanently increases what you’re able to do\n\nThe fog – why this works ?\n\nYou typically can’t think your way to experience-based information.\nImagine you’re in a thick fog where you can only see two steps ahead. You can’t just stare into it longer and expect to see further. Only after moving forward a few steps will you see what it concealed.\nThis is why overperfecting before taking action is an antipattern; you’re trying to see 10 moves deep into the fog by squinting harder, rather than taking a single step forward, usually out of fear.\n\nIts about experiences, NOT time\n\nGaining experience isn’t a function of time, but rather how many new experiences you have.\nif you don’t have new experiences, you will not gain experience-based knowledge\nif you have only a few new experiences, you will gain a bit of experience-based knowledge\nif you have a lot of new experiences, you will gain a lot of experience-based knowledge\nTo exploit this, get in the habit of maximizing the amount of new experiences you have.\n\nExperience-based knowledge is valuable, but why?\n\nMaybe because it takes energy to get (doing things, overcoming fears, overcoming fixed-ness), so it filters many people out of knowing it, therefore giving whoever has it a large competitive advantage.\n\n\n\nIdeas for how to do this\n\nThe idea is to do lots of new, unfamiliar stuff that falls outside of the distribution of what you typically do.\n\n\nDo something you’ve already done, but more extreme\n\ncreate a much bigger project than you have before\ntry working for a longer stretch of time than you’re used to\netc\n\nDo something of a very different nature from what you’ve already done\n\nmake a project that requires skills you don’t have but could get\ntake an action that puts you in a new environment or context\ndo something you’ve been afraid to try for a while, that you usually find complex ways around doing\netc\n\n\n\nBe creative with coming up with new ways to do new things. It’s a skill you can improve through practice.\n\n\n\nSpecific Examples of Uses\nTremendously increase your skillset by speedrunning as many out-of-personal-distribution projects as possible.\n\nCompleting projects 15% outside of your skillset expands your skillset by 15%. Using the rule of 72, this doubles your skillset every 5 projects.\nthe projects take more time every iteration, but you also get faster at doing them.\nSpeedrun this to get really, really good at programming.\nAs a side effect, you’ll build up a massive portfolio. This can help you land jobs, especially if the projects require the same tools/skills you’d use on the job you’re applying for. Make sure to actually finish projects. Avoid ‘shiny object syndrome’.\n\nShiny object syndrome refers to the tendency to constantly chase after new, exciting opportunities or ideas while abandoning previous projects before completing them.\n\n\n\n\nCuring/preventing overfitting\n\nOverfitting is when your mental models become aligned with your limited sample of experiences to the detriment of their alignment with the real world.\n![[Pasted image 20250703092203.png]]\nThis can happen if all you do is read information on the internet about how the world is (which, in moderation, is useful!) without going outside and getting information from new, different experiences.\nOverfitting can cause (and be caused by) irrational fear\n\nOverfitting can have negative consequences, such as overblown fear, and fear of things you shouldn’t be afraid of.\n\nPreventing/curing irrational fears\n\nIn ML, adding more training data is a common strategy to prevent models from overfitting.\nThere are two main tactics that have helped me overcome overfitting-based fear. The first is to come to terms with the risks. The second is to touch grass. This is the main idea behind exposure therapy – get experience related to the fears you know are irrational, and they will dissolve"
  },
  {
    "objectID": "private/mle-guide/index.html#how-to-get-maximum-knowledge-wrt-time",
    "href": "private/mle-guide/index.html#how-to-get-maximum-knowledge-wrt-time",
    "title": "The Cracked MLE Guide",
    "section": "How to get maximum knowledge wrt time",
    "text": "How to get maximum knowledge wrt time\n\nThe most valuable kind of knowledge is the type you get from experience. So how do you get a ton of it?\nAnswer : Maximize doing new unfamiliar things. You’ll get lots of new, useful, experience-based knowledge. The kind you get from years of experience. The kind you cant find on Google.\nExperience comes from experiencing things, not from time. If you passively wait around, you’ll barely get any. The opposite is true, too – if you actively seek out experience, you’ll get way more of it.\nExperience-based info\n\nExperience-based info is the extremely valuable type of info you get from doing new, different things.\n![[Pasted image 20250703091152.png]]\n\nSome forms of experience-based info you might learn from new things:\n\nuseful details that fill in a map of the territory your business operates in, showing you better directions to move in (this is why pivots happen)\nknowledge that a specific fear you had is not actually anything to be afraid of\na skill (knowledge of how to do something) that permanently increases what you’re able to do\n\nThe fog – why this works ?\n\nYou typically can’t think your way to experience-based information.\nImagine you’re in a thick fog where you can only see two steps ahead. You can’t just stare into it longer and expect to see further. Only after moving forward a few steps will you see what it concealed.\nThis is why overperfecting before taking action is an antipattern; you’re trying to see 10 moves deep into the fog by squinting harder, rather than taking a single step forward, usually out of fear.\n\nIts about experiences, NOT time\n\nGaining experience isn’t a function of time, but rather how many new experiences you have.\nif you don’t have new experiences, you will not gain experience-based knowledge\nif you have only a few new experiences, you will gain a bit of experience-based knowledge\nif you have a lot of new experiences, you will gain a lot of experience-based knowledge\nTo exploit this, get in the habit of maximizing the amount of new experiences you have.\n\nExperience-based knowledge is valuable, but why?\n\nMaybe because it takes energy to get (doing things, overcoming fears, overcoming fixed-ness), so it filters many people out of knowing it, therefore giving whoever has it a large competitive advantage.\n\n\n\nIdeas for how to do this\n\nThe idea is to do lots of new, unfamiliar stuff that falls outside of the distribution of what you typically do.\n\n\nDo something you’ve already done, but more extreme\n\ncreate a much bigger project than you have before\ntry working for a longer stretch of time than you’re used to\netc\n\nDo something of a very different nature from what you’ve already done\n\nmake a project that requires skills you don’t have but could get\ntake an action that puts you in a new environment or context\ndo something you’ve been afraid to try for a while, that you usually find complex ways around doing\netc\n\n\n\nBe creative with coming up with new ways to do new things. It’s a skill you can improve through practice.\n\n\n\nSpecific Examples of Uses\nTremendously increase your skillset by speedrunning as many out-of-personal-distribution projects as possible.\n\nCompleting projects 15% outside of your skillset expands your skillset by 15%. Using the rule of 72, this doubles your skillset every 5 projects.\nthe projects take more time every iteration, but you also get faster at doing them.\nSpeedrun this to get really, really good at programming.\nAs a side effect, you’ll build up a massive portfolio. This can help you land jobs, especially if the projects require the same tools/skills you’d use on the job you’re applying for. Make sure to actually finish projects. Avoid ‘shiny object syndrome’.\n\nShiny object syndrome refers to the tendency to constantly chase after new, exciting opportunities or ideas while abandoning previous projects before completing them.\n\n\n\n\nCuring/preventing overfitting\n\nOverfitting is when your mental models become aligned with your limited sample of experiences to the detriment of their alignment with the real world.\nThis can happen if all you do is read information on the internet about how the world is (which, in moderation, is useful!) without going outside and getting information from new, different experiences.\nOverfitting can cause (and be caused by) irrational fear\n\nOverfitting can have negative consequences, such as overblown fear, and fear of things you shouldn’t be afraid of.\n\nPreventing/curing irrational fears\n\nIn ML, adding more training data is a common strategy to prevent models from overfitting.\nThere are two main tactics that have helped me overcome overfitting-based fear. The first is to come to terms with the risks. The second is to touch grass. This is the main idea behind exposure therapy – get experience related to the fears you know are irrational, and they will dissolve"
  },
  {
    "objectID": "private/mle-guide/index.html#how-i-cured-my-procrastination",
    "href": "private/mle-guide/index.html#how-i-cured-my-procrastination",
    "title": "The Cracked MLE Guide",
    "section": "How I Cured My Procrastination",
    "text": "How I Cured My Procrastination\n\nThe method is to get in the habit of starting everything immediately and chunking it out so you finish early.\n\nThe goal is to repeatedly show your brain that work is awesome, so it likes it and goes for it. Work should be a long, warm bath, not a burn on a hot stove.\n1. Get in the habit of starting everything immediately\n2. Break every medium-to-large task into chunks over time, and finish early\n\nStart every task immediately, and if it’s medium-to-large sized, break it up into chunks (of meaningful size). Do one chunk every day, or week, whatever you choose, so that you finish early.\n if your chunks are too small to make meaningful progress, you won’t feel much of a reward. The chunks don’t have to be hard! They just have to make you feel like you really truly got something done.\n\n3. STOP after you finish a chunk &lt; most imp\n\nYou will likely not want to stop after you complete a chunk. After all why shouldn’t you? Why not keep going and finish the whole paper right now while you’re in the zone?\nBecause not getting what you want builds anticipation. And this further conditions your brain to want work and the rewards that come with it. The goal isn’t to get the work done, it’s to cure procrastination so you can have tons of freedom and live out an adventure. Stop. After. Each. Chunk. Stop. Seriously.\n\n4. Continue doing this, forever\n\nEven if you stop doing it for periods of time, get back into it, even if its been months."
  },
  {
    "objectID": "private/mle-guide/index.html#how-to-overcome-plateaus",
    "href": "private/mle-guide/index.html#how-to-overcome-plateaus",
    "title": "The Cracked MLE Guide",
    "section": "How to overcome plateaus",
    "text": "How to overcome plateaus\n\nA great way to break through plateaus is to improve fundamentals. Either improve ones you suck at, or find new ones you’re not aware of and get good at them.\novercome plateaus by getting large improvements from finding (and then training) core concepts you’re currently overlooking.\nWhat are Fundamentals ?\n\nFor any concept, the fundamentals are the subconcepts make up large fractions of it.\nGenerally, learning them provides the fastest increases in competency.\nAmdahl’s Law: Improvement to a system’s subcomponent can improve the whole system by how much of a fraction the subcomponent is.\n\nSome fundamentals are very obvious. Others are extremely hard to notice. Even though they’re right under your nose.\n\nGet skilled at finding core concepts, and then target them for the fastest improvement in learning.\n\nStrengthening fundamentals can be done recursively, because each concept is composed of subconcepts, whose impact seems to (from my observation) have a power law distribution (meaning, the 80/20 Pareto principle works on them).\nYou may find that sometimes, your concept is 90% composed of a subconcept 10 layers of recursion down.\n\nExamples of easy to find fundamentals\n\nWriting examples: have a title, have good grammar, write something interesting\nProgramming examples: programming language, common algorithms/data structures, time and space complexity, ML/DL fundamental concepts used everywhere…\n\nExamples of hard to see core concepts\n\nProgramming examples: building new things to learn new skills, learning to navigate up/down abstractions quickly etc..\nTechniques to find overlooked fundamentals\n\nListen for words experts commonly use, then double down on understanding those concepts\nRead Fundamental Books.\nBuild Something from scratch.\n\n\nZeroth principle analysis\n\nRecursively break a concept into its subcomponents by asking ‘origin questions’ and putting energy into figuring out specific and actionable answers, rather than general useless ones.\nIt’s just first principles analysis, but you don’t stop going deeper when the questions get extremely tough to give specific, actionable answers to.\n\nWhat are ‘origin questions’?\n\nOrigin questions are any question that probes into the nature of there a thing came from. “Why”, “how”, “what”, etc"
  },
  {
    "objectID": "private/mle-guide/index.html#how-to-understandretain-complex-concepts-10x-better",
    "href": "private/mle-guide/index.html#how-to-understandretain-complex-concepts-10x-better",
    "title": "The Cracked MLE Guide",
    "section": "How to understand/retain complex concepts 10x better ?",
    "text": "How to understand/retain complex concepts 10x better ?\nThe whole idea: compress the concept by repeatedly practicing explaining it until you understand it from first principles\n\nTry to explain the concept as best as you can (ideally out loud, as this is much more effective). Pretend you’re having an imaginary conversation where you are explaining the concept to someone else.\nWhile explaining the concept, focus on making it easier for them to understand.\nRepeat this over and over, iteratively refining your explanation of the concept to give increasingly compressed and effective explanations. (Ideally continue until you can explain it from first principles only)\n\n\nOver several repetitions, you will end up with a significantly simpler understanding of the concept than when you started, allowing you to mentally work with it much more efficiently. Use this to get an extremely effective understanding of any concept.\nFor a new concept, the first time you do this will be the hardest. but it gets easier after very few repetitions. you’ll quickly see it becoming easier after the first few iterations.\nYou will hit road bumps when teaching/explaining. These will require you to figure out questions about the concept you’re learning. Part of the process is answering these questions, however long it takes to figure out how to answer it with specificity from first principles\nExample &gt;&gt;  I read an article called The First Law of Complexodynamics. In the article there was the concept of the ‘sophistication’ of a string. This was very foreign to me, and hard for me to understand, but I gained a decent understanding of it by using this (Explaining till First Principles) technique for about 20 iterations. Eventually I went from barely being able to understand it to being able to teach it to my dad, who is not a computer scientist and has no idea who Kolmogorov is or what strings are."
  },
  {
    "objectID": "private/mle-guide/index.html#a-habit-to-make-learning-much-more-fun-and-effective",
    "href": "private/mle-guide/index.html#a-habit-to-make-learning-much-more-fun-and-effective",
    "title": "The Cracked MLE Guide",
    "section": "A habit to make learning much more fun and effective",
    "text": "A habit to make learning much more fun and effective\nThe habit:\n\nEvery time you hear a new term/concept, look it up immediately\nAs you read about it, you’ll form questions and find more new, unfamiliar ideas/terms. Look up the ones that interest you the most\nRepeat (recursive)\n\n\n\nGoing down a rabbit hole for every unfamiliar concept (or any concept you choose, really) will lead you to learn a ton and have a lot of fun. This will make you remember the initial concept waaaay better, with more detail/meaning, and you’ll learn more than just that one concept.\nIt’ll also (hopefully) make you enjoy learning more (in general), which is a huge long-term advantage that will make you learn more effectively.\nInterest is a huge learning advantage. Fostering interest is one of the most powerful ways to get good at something over the long run.\n\nWhen you get very interested in something, your mind gets better at understanding it in the background. It’s almost like your mind becomes altered to be better at learning it."
  }
]